{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled3.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7yYE7X7_mgS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os, pickle, re, sklearn, string\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "44cc93e7-b21f-49c2-ede2-4150de66e298",
        "id": "177cZi2OjAWT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')# Download text data sets, including stop words\n",
        "from nltk.corpus import stopwords # Import the stop word list\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "\n",
        "# Download the punkt tokenizer for sentence splitting\n",
        "import nltk.data\n",
        "nltk.download('punkt')   \n",
        "\n",
        "# Load the punkt tokenizer\n",
        "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "\n",
        "class Creat_vocabulary():\n",
        "  \n",
        "  def __init__(self):\n",
        "    self.sentences = []\n",
        "    self.word2idx = {}\n",
        "    self.idx2word = {}\n",
        "    self.idx_pairs = []\n",
        "    self.all_voc = []\n",
        "    self.negative_pairs = []\n",
        "    \n",
        "    \n",
        "  def get_sentence(self):\n",
        "    \n",
        "      return self.sentences\n",
        "    \n",
        "    \n",
        "  def get_word2idx(self):\n",
        "    \n",
        "      return self.word2idx\n",
        "  \n",
        "  \n",
        "  def get_idx2word(self):\n",
        "    \n",
        "      return self.idx2word\n",
        "  \n",
        "  \n",
        "  def get_idx_pairs(self):\n",
        "    \n",
        "      return self.idx_pairs\n",
        "    \n",
        "  def expand(self,ListofList):\n",
        "    \n",
        "    for ll in ListofList:\n",
        "      for l in ll:\n",
        "        self.all_voc.append(l)\n",
        "    return self.all_voc   \n",
        "  \n",
        "\n",
        "  # Define a function to split a review into parsed sentences\n",
        "  def review_to_sentences( self, review, tokenizer, remove_stopwords=False ):\n",
        "      # Function to split a review into parsed sentences. Returns a \n",
        "      # list of sentences, where each sentence is a list of words\n",
        "    \n",
        "      # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
        "      raw_sentences = tokenizer.tokenize(review.strip())\n",
        "    \n",
        "    \n",
        "      # 2. Loop over each sentence\n",
        "      sentences = []\n",
        "      for raw_sentence in raw_sentences:\n",
        "      \n",
        "          # If a sentence is empty, skip it\n",
        "          if len(raw_sentence) > 0:\n",
        "          \n",
        "              # Otherwise, call review_to_wordlist to get a list of words\n",
        "              sentences.append( self.review_to_wordlist( raw_sentence,remove_stopwords ))\n",
        "    \n",
        "      # Return List os List of Tokens in a sentence\n",
        "      return sentences\n",
        "\n",
        "  \n",
        "  def review_to_wordlist(self, review, remove_stopwords=False ):\n",
        "  \n",
        "    # Function to convert a document to a sequence of words,\n",
        "    # optionally removing stop words. Returns a list of words.\n",
        "\n",
        "    # 1. Remove HTML\n",
        "    review_text = BeautifulSoup(review).get_text()\n",
        "\n",
        "    # 2. Remove non-letters\n",
        "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
        "\n",
        "    # 3. Convert words to lower case and split them\n",
        "    words = review_text.lower().split()\n",
        "\n",
        "    # 4. Optionally remove stop words (false by default)\n",
        "    if remove_stopwords:\n",
        "        stops = set(stopwords.words(\"english\"))\n",
        "        words = [w for w in words if not w in stops]\n",
        "\n",
        "    # 5. Return a list of words\n",
        "    return(words)\n",
        "\n",
        "\n",
        "  def reviews_to_Words(self,review, tokenizer):\n",
        "  \n",
        "    sentences = []  # Initialize an empty list of sentences\n",
        "\n",
        "    print (\"Parsing sentences from training set\")\n",
        "  \n",
        "    for review in data1[\"review\"]:\n",
        "      self.sentences += self.review_to_sentences(review, tokenizer)\n",
        "  \n",
        "    return self.sentences\n",
        "  \n",
        " \n",
        "  \n",
        "  def dictionary(self,review, tokenizer ):\n",
        "    \n",
        "    vocabulary = []\n",
        "    for sentence in self.sentences:\n",
        "      for token in sentence:\n",
        "          if token not in vocabulary:\n",
        "              vocabulary.append(token)\n",
        "\n",
        "    self.word2idx = {w: idx for (idx, w) in enumerate(vocabulary)}\n",
        "    self.idx2word = {idx: w for (idx, w) in enumerate(vocabulary)}\n",
        "\n",
        "    vocabulary_size = len(vocabulary)\n",
        "    \n",
        "    return vocabulary_size, self.word2idx, self.idx2word\n",
        "  \n",
        "  def skip_contex_tuple(self,context_size):\n",
        "    \n",
        "    \n",
        "    # changing context_size to window size\n",
        "    if(context_size % 2 != 0):\n",
        "      window_size = int(context_size + 1)//2\n",
        "    else:\n",
        "      window_size = int(context_size)//2\n",
        "  \n",
        "    #find context words around each word in a sentence and then coolect it in a list using vocab index in our dic\n",
        "    for sentence in self.sentences:\n",
        "      \n",
        "      \n",
        "      indices = [self.word2idx[word] for word in sentence]\n",
        "      \n",
        "      # for each word, threated as center word\n",
        "      for center_word_pos in range(len(indices)):\n",
        "        tup =[]\n",
        "        \n",
        "        for w in range(-window_size, window_size):\n",
        "          cntx_pos = center_word_pos + w \n",
        "          \n",
        "\n",
        "          if cntx_pos <=0 or cntx_pos>=len(indices) or cntx_pos == center_word_pos:\n",
        "\n",
        "            continue\n",
        "            \n",
        "          tup.append(self.word2idx[sentence[cntx_pos]])\n",
        "            \n",
        "        # for each window position\n",
        "        #tup = [sentence[center_word_pos + w ] for w in range(-window_size, window_size) if center_word_pos + w > 0 and center_word_pos + w <= len(indices) and center_word_pos  != center_word_pos + w ]\n",
        "\n",
        "        self.idx_pairs.append((indices[center_word_pos], tup))\n",
        "\n",
        "    self.idx_pairs = np.array(self.idx_pairs) # it will be useful to have this as numpy array\n",
        "    \n",
        "    return self.idx_pairs\n",
        "  \n",
        "  \n",
        "  def get_negatives(self,idx_pairs, sampling_weights, K):\n",
        "    \n",
        "    all_negatives = []\n",
        "    \n",
        "    generator = RandomGenerator(sampling_weights)\n",
        "    \n",
        "    for pair in idx_pairs:\n",
        "        negatives = []\n",
        "        contexts = pair[1]\n",
        "        center = pair[0]\n",
        "        while len(negatives) < len(contexts) * K:\n",
        "          \n",
        "            neg = generator.draw()\n",
        "            \n",
        "            # Noise words cannot be context words\n",
        "            if neg not in contexts:\n",
        "              \n",
        "                negatives.append(neg)\n",
        "                \n",
        "        self.negative_pairs.append((center, negatives))\n",
        "     \n",
        "    self.negative_pairs = np.array(self.negative_pairs)\n",
        "        \n",
        "    return self.negative_pairs"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTpxAtjA55Bn",
        "colab_type": "code",
        "outputId": "fc2c50e9-57ac-4b87-83c3-8ae5e9689d1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "source": [
        "for conext in idx_pairs:\n",
        "  print(context)\n",
        "  print(conext[1])\n",
        "  break"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-165f08463d96>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mconext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0midx_pairs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'idx_pairs' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKirWfMclgJC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "outputId": "baf07166-ce38-4c1a-9827-19f48c9633de"
      },
      "source": [
        "a = vocab.get_word2idx()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-1e9ec13a5dbe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_word2idx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'vocab' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wm_O-Q0Blk-G",
        "colab_type": "code",
        "outputId": "2eeaa1bc-e5c8-4e13-c5c1-2ac170d70baf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        }
      },
      "source": [
        "a[\"all\"]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-3be88b58e3f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"all\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'a' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njWHoCLZV8DO",
        "colab_type": "code",
        "outputId": "02306e0a-e955-4a3e-d288-993fc3cc776f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "\n",
        "import random\n",
        "\n",
        "class RandomGenerator(object):\n",
        "    \"\"\"Draw a random int in [0, n] according to n sampling weights\"\"\"\n",
        "    def __init__(self, sampling_weights):\n",
        "        self.population = list(range(len(sampling_weights)))\n",
        "        self.sampling_weights = sampling_weights\n",
        "        self.candidates = []\n",
        "        self.i = 0\n",
        "\n",
        "    def draw(self):\n",
        "        if self.i == len(self.candidates):\n",
        "            self.candidates = random.choices(\n",
        "                self.population, self.sampling_weights, k=10000)\n",
        "            self.i = 0\n",
        "        self.i += 1\n",
        "        return self.candidates[self.i-1]\n",
        "\n",
        "      \n",
        "      \n",
        "      \n",
        "\n",
        "generator = RandomGenerator([2,3,4])\n",
        "[generator.draw() for _ in range(10)]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 2, 1, 2, 2, 1, 2, 2, 2, 2]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOW4cO-rWNZO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLymiuSicQGL",
        "colab_type": "code",
        "outputId": "9c33e06e-79aa-4a8e-9546-a5fbbe49e7dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "import glob\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "#in order to use the dataset from MyDrive, need to mount it in google colab\n",
        "drive.mount('/content/gdrive/')\n",
        "path = '/content/gdrive/My Drive/data/labeledTrainData.tsv'\n",
        "\n",
        "data1 = pd.read_csv(path, header=0, delimiter=\"\\t\", quoting=3)\n",
        "data = data1['review']\n",
        "target = data1['sentiment']"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybSrRiltdYO2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn import datasets, linear_model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# create training and testing vars\n",
        "train, test, ytrain, ytest = train_test_split(data, target, test_size=0.2)\n",
        "#print X_train.shape, y_train.shape\n",
        "#print X_test.shape, y_test.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8IIPXlQXy4Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xtrain = train.values.reshape(-1,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPA-qWaOWT2p",
        "colab_type": "code",
        "outputId": "9a348417-7a5b-404e-9dd4-9b3f9f648967",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "sentences = []  # Initialize an empty list of sentences\n",
        "vocab =  Creat_vocabulary()\n",
        "sentences =vocab.reviews_to_Words(data1 , tokenizer)\n",
        "\n",
        "vocabulary_size, word2idx, idx2word = vocab.dictionary(data1 , tokenizer )"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parsing sentences from training set\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:273: UserWarning: \"b'.'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
            "  ' Beautiful Soup.' % markup)\n",
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:336: UserWarning: \"http://www.happierabroad.com\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKQe2rP6NS2_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = sentences[2]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWFEWMgKNXO8",
        "colab_type": "code",
        "outputId": "5a6854f2-a1ab-4f23-b4c3-6d4c273bfd82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "a[1]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'is'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAJzUW4ki-C_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "idx_pairs = vocab.skip_contex_tuple(3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3CH4U1N4IUW",
        "colab_type": "code",
        "outputId": "b416d9ff-a6d5-42d2-c67a-493d7b193cb0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "np.where(idx_pairs == 27)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([     32,      55,     250, ..., 5910728, 5912708, 5920634]),\n",
              " array([0, 0, 0, ..., 0, 0, 0]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRC9msUQcOXn",
        "colab_type": "code",
        "outputId": "be3b1eea-ca66-40ef-889b-c54864b519b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "idx_pairs"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, list([1])],\n",
              "       [1, list([2])],\n",
              "       [2, list([1, 3])],\n",
              "       ...,\n",
              "       [131, list([1189, 62, 31])],\n",
              "       [31, list([62, 131, 3280])],\n",
              "       [3280, list([131, 31])]], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxPk0rG09pNx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umPhbsODV_iH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import collections\n",
        "\n",
        "listofV= vocab.get_sentence()\n",
        "\n",
        "counter = collections.Counter(vocab.expand(listofV))\n",
        "\n",
        "\n",
        "counters = list(counter.values())\n",
        "\n",
        "\n",
        "sampling_weights = [counters[i]**0.75 for i in range(len(counters))]\n",
        "\n",
        "vocab_size = len(counter)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gceLeFbPYW46",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "all_negatives = vocab.get_negatives(idx_pairs, sampling_weights, 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtxGBTqMBB7A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f72d271b-73f0-4635-9fb3-f2a2a22d722a"
      },
      "source": [
        "vocab_size = len(counter)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "74218"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hbiXiRAtgeY",
        "colab_type": "code",
        "outputId": "e702cc7f-310e-4fd7-809f-e1bbc39ed09d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "idx_pairs.shape"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5920724, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pV2SHWo6C7s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CONTEXT_SIZE = len(all_negatives) + len(idx_pairs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vU_XKfD93w5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e179f180-fce2-46ac-b8ac-15b3f63b591b"
      },
      "source": [
        "len(idx_pairs)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5920724"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOpZiGVcCWJv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86ta_1Nc59jG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "7b6893cd-3c43-4e1e-b1a2-92d2e22fb214"
      },
      "source": [
        "\n",
        "class Skipgram(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
        "      \n",
        "        super(SkipgramModeler, self).__init__()\n",
        "        \n",
        "        \n",
        "        self.embed_v = nn.Embedding(vocab_size, embedding_dim)\n",
        "        \n",
        "        self.embed_u = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        \n",
        "        #self.linear1 = nn.Linear(embedding_dim, 128)\n",
        "        #self.linear2 = nn.Linear(128, context_size * vocab_size)\n",
        "        #self.parameters['context_size'] = context_size\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        '''\n",
        "        embeds = self.embeddings(inputs).view((1, -1))  # -1 implies size inferred for that index from the size of the data\n",
        "        #print(np.mean(np.mean(self.linear2.weight.data.numpy())))\n",
        "        out1 = F.relu(self.linear1(embeds)) # output of first layer\n",
        "        out2 = self.linear2(out1)           # output of second layer\n",
        "        #print(embeds)\n",
        "        log_probs = F.log_softmax(out2, dim=1).view(CONTEXT_SIZE,-1)\n",
        "        '''\n",
        "        return NULL\n",
        "\n",
        "    def predict(self,center, contexts_and_negatives, embed_v, embed_u):\n",
        "      \n",
        "        v = embed_v(center)\n",
        "        u = embed_u(contexts_and_negatives)\n",
        "        pred = nd.batch_dot(v, u.swapaxes(1, 2))\n",
        "        log_probs = F.log_softmax(out2, dim=1).view(CONTEXT_SIZE,-1)\n",
        "\n",
        "        return log_probs\n",
        "\n",
        "    def freeze_layer(self,layer):\n",
        "        for name,child in model.named_children():\n",
        "            print(name,child)\n",
        "            if(name == layer):\n",
        "                for names,params in child.named_parameters():\n",
        "                    print(names,params)\n",
        "                    print(params.size())\n",
        "                    params.requires_grad= False\n",
        "\n",
        "    def print_layer_parameters(self):\n",
        "        for name,child in model.named_children():\n",
        "                print(name,child)\n",
        "                for names,params in child.named_parameters():\n",
        "                    print(names,params)\n",
        "                    print(params.size())\n",
        "\n",
        "    def write_embedding_to_file(self,filename):\n",
        "        for i in self.embeddings.parameters():\n",
        "            weights = i.data.numpy()\n",
        "        np.save(filename,weights)\n",
        "\n",
        "\n",
        "losses = []\n",
        "EMBEDDING_DIM = 200\n",
        "CONTEXT_SIZE = len(all_negatives) + len(idx_pairs)\n",
        "loss_function = nn.NLLLoss()\n",
        "model = SkipgramModeler(vocab_size, EMBEDDING_DIM, CONTEXT_SIZE)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-6bd26b28fddb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mSkipgram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUJVQyAjLH1R",
        "colab_type": "code",
        "outputId": "c319c64a-d992-40a7-e9ef-4b3c6b54c7cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        }
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "\n",
        "\n",
        "'''\n",
        "context_size = 3\n",
        "embedding_dim = 126\n",
        "\n",
        "model = nn.Sequential(\n",
        "    \n",
        "    \n",
        "                      nn.Embedding(vocabulary_size, embedding_dim),\n",
        "                      nn.Linear(embedding_dim,256),\n",
        "                      nn.Linear(256,context_size * vocabulary_size),\n",
        "                      nn.LogSoftmax(dim=1))\n",
        "\n",
        "\n",
        "\n",
        "objective = nn.NLLLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.003)\n",
        "'''\n",
        "\n",
        "\n",
        "losses= []\n",
        "epoch = 4\n",
        "\n",
        "for e in range(epoch):\n",
        "  \n",
        "  running_loss = 0\n",
        "\n",
        "  for i in range(0,len(idx_pairs)):\n",
        "    \n",
        "    for context in idx_pairs[i][1]\n",
        "      \n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(context_idxs)\n",
        "        \n",
        "        loss = objective(output, 1, dtype=torch.long))\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "        losses.append(loss.item())\n",
        "        \n",
        "        \n",
        "        \n",
        "    for negative in all_negatives[i][1]:\n",
        "      \n",
        "       optimizer.zero_grad()\n",
        "\n",
        "        output = model(context_idxs)\n",
        "        \n",
        "        loss = objective(output, 0, dtype=torch.long))\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "        losses.append(loss.item())\n",
        "      \n",
        "  else:\n",
        "        print(f\"Training loss: {running_loss/len(trainloader)}\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-11-2b258ec15c5c>\"\u001b[0;36m, line \u001b[0;32m32\u001b[0m\n\u001b[0;31m    for context_idxs in\u001b[0m\n\u001b[0m                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13sZTA9ECu9w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "2f8a0e04-c922-4554-f7dd-3b9773ec4c5f"
      },
      "source": [
        "for i in idx_pairs[2][1]:\n",
        "  print(i)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnFRlwSpO3CY",
        "colab_type": "code",
        "outputId": "80ad7066-2c2c-49ae-82ad-af07547307e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2028
        }
      },
      "source": [
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',    level=logging.INFO)\n",
        "\n",
        "def myhashfxn(obj):\n",
        "    return hash(obj) % (2 ** 32)\n",
        "\n",
        "# Set values for various parameters\n",
        "num_features = 400    # Word vector dimensionality                      \n",
        "min_word_count = 60   # Minimum word count                        \n",
        "num_workers = 4       # Number of threads to run in parallel\n",
        "context = 10          # Context window size                                                                                    \n",
        "downsampling = 1e-3   # Downsample setting for frequent words\n",
        "\n",
        "\n",
        "#**********************************************************\n",
        "\n",
        "\n",
        "\n",
        "# Initialize and train the model (this will take some time)\n",
        "from gensim.models import word2vec\n",
        "#model = word2vec.Word2Vec(hashfxn=myhashfxn)\n",
        "\n",
        "print (\"Training model...\")\n",
        "model = word2vec.Word2Vec(sentences, workers=num_workers,             size=num_features, min_count = min_word_count,             window = context, sample = downsampling,hashfxn=myhashfxn)\n",
        "\n",
        "# If you don't plan to train the model any further, calling \n",
        "# init_sims will make the model much more memory-efficient.\n",
        "model.init_sims(replace=True)\n",
        "\n",
        "# It can be helpful to create a meaningful model name and \n",
        "# save the model for later use. You can load it later using Word2Vec.load()\n",
        "model_name = \"300features_40minwords_10context\"\n",
        "model.save(model_name)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-06-06 02:15:44,719 : INFO : 'pattern' package not found; tag filters are not available for English\n",
            "2019-06-06 02:15:44,729 : INFO : collecting all words and their counts\n",
            "2019-06-06 02:15:44,730 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2019-06-06 02:15:44,786 : INFO : PROGRESS: at sentence #10000, processed 225803 words, keeping 17776 word types\n",
            "2019-06-06 02:15:44,845 : INFO : PROGRESS: at sentence #20000, processed 451892 words, keeping 24948 word types\n",
            "2019-06-06 02:15:44,901 : INFO : PROGRESS: at sentence #30000, processed 671315 words, keeping 30034 word types\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training model...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-06-06 02:15:44,963 : INFO : PROGRESS: at sentence #40000, processed 897815 words, keeping 34348 word types\n",
            "2019-06-06 02:15:45,019 : INFO : PROGRESS: at sentence #50000, processed 1116963 words, keeping 37761 word types\n",
            "2019-06-06 02:15:45,073 : INFO : PROGRESS: at sentence #60000, processed 1338404 words, keeping 40723 word types\n",
            "2019-06-06 02:15:45,127 : INFO : PROGRESS: at sentence #70000, processed 1561580 words, keeping 43333 word types\n",
            "2019-06-06 02:15:45,181 : INFO : PROGRESS: at sentence #80000, processed 1780887 words, keeping 45714 word types\n",
            "2019-06-06 02:15:45,235 : INFO : PROGRESS: at sentence #90000, processed 2004996 words, keeping 48135 word types\n",
            "2019-06-06 02:15:45,289 : INFO : PROGRESS: at sentence #100000, processed 2226966 words, keeping 50207 word types\n",
            "2019-06-06 02:15:45,342 : INFO : PROGRESS: at sentence #110000, processed 2446580 words, keeping 52081 word types\n",
            "2019-06-06 02:15:45,397 : INFO : PROGRESS: at sentence #120000, processed 2668775 words, keeping 54119 word types\n",
            "2019-06-06 02:15:45,452 : INFO : PROGRESS: at sentence #130000, processed 2894303 words, keeping 55847 word types\n",
            "2019-06-06 02:15:45,504 : INFO : PROGRESS: at sentence #140000, processed 3107005 words, keeping 57346 word types\n",
            "2019-06-06 02:15:45,558 : INFO : PROGRESS: at sentence #150000, processed 3332627 words, keeping 59055 word types\n",
            "2019-06-06 02:15:45,613 : INFO : PROGRESS: at sentence #160000, processed 3555315 words, keeping 60617 word types\n",
            "2019-06-06 02:15:45,666 : INFO : PROGRESS: at sentence #170000, processed 3778655 words, keeping 62077 word types\n",
            "2019-06-06 02:15:45,718 : INFO : PROGRESS: at sentence #180000, processed 3999236 words, keeping 63496 word types\n",
            "2019-06-06 02:15:45,771 : INFO : PROGRESS: at sentence #190000, processed 4224449 words, keeping 64794 word types\n",
            "2019-06-06 02:15:45,828 : INFO : PROGRESS: at sentence #200000, processed 4448603 words, keeping 66087 word types\n",
            "2019-06-06 02:15:45,881 : INFO : PROGRESS: at sentence #210000, processed 4669967 words, keeping 67390 word types\n",
            "2019-06-06 02:15:45,934 : INFO : PROGRESS: at sentence #220000, processed 4894968 words, keeping 68697 word types\n",
            "2019-06-06 02:15:45,988 : INFO : PROGRESS: at sentence #230000, processed 5117545 words, keeping 69958 word types\n",
            "2019-06-06 02:15:46,042 : INFO : PROGRESS: at sentence #240000, processed 5345050 words, keeping 71167 word types\n",
            "2019-06-06 02:15:46,097 : INFO : PROGRESS: at sentence #250000, processed 5559165 words, keeping 72351 word types\n",
            "2019-06-06 02:15:46,151 : INFO : PROGRESS: at sentence #260000, processed 5779146 words, keeping 73478 word types\n",
            "2019-06-06 02:15:46,188 : INFO : collected 74218 word types from a corpus of 5920724 raw words and 266551 sentences\n",
            "2019-06-06 02:15:46,189 : INFO : Loading a fresh vocabulary\n",
            "2019-06-06 02:15:46,239 : INFO : effective_min_count=60 retains 6278 unique words (8% of original 74218, drops 67940)\n",
            "2019-06-06 02:15:46,240 : INFO : effective_min_count=60 leaves 5461149 word corpus (92% of original 5920724, drops 459575)\n",
            "2019-06-06 02:15:46,261 : INFO : deleting the raw counts dictionary of 74218 items\n",
            "2019-06-06 02:15:46,264 : INFO : sample=0.001 downsamples 50 most-common words\n",
            "2019-06-06 02:15:46,265 : INFO : downsampling leaves estimated 3933998 word corpus (72.0% of prior 5461149)\n",
            "2019-06-06 02:15:46,282 : INFO : estimated required memory for 6278 words and 400 dimensions: 23228600 bytes\n",
            "2019-06-06 02:15:46,282 : INFO : resetting layer weights\n",
            "2019-06-06 02:15:46,365 : INFO : training model with 4 workers on 6278 vocabulary and 400 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
            "2019-06-06 02:15:47,385 : INFO : EPOCH 1 - PROGRESS: at 10.38% examples, 406628 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:15:48,408 : INFO : EPOCH 1 - PROGRESS: at 21.29% examples, 413575 words/s, in_qsize 7, out_qsize 1\n",
            "2019-06-06 02:15:49,410 : INFO : EPOCH 1 - PROGRESS: at 32.44% examples, 421396 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:15:50,432 : INFO : EPOCH 1 - PROGRESS: at 43.62% examples, 423259 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:15:51,464 : INFO : EPOCH 1 - PROGRESS: at 54.97% examples, 424817 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:15:52,484 : INFO : EPOCH 1 - PROGRESS: at 66.10% examples, 425588 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:15:53,492 : INFO : EPOCH 1 - PROGRESS: at 77.17% examples, 426817 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:15:54,514 : INFO : EPOCH 1 - PROGRESS: at 87.80% examples, 425321 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:15:55,526 : INFO : EPOCH 1 - PROGRESS: at 99.28% examples, 426877 words/s, in_qsize 5, out_qsize 0\n",
            "2019-06-06 02:15:55,543 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2019-06-06 02:15:55,556 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2019-06-06 02:15:55,571 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2019-06-06 02:15:55,572 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-06-06 02:15:55,573 : INFO : EPOCH - 1 : training on 5920724 raw words (3934036 effective words) took 9.2s, 427695 effective words/s\n",
            "2019-06-06 02:15:56,590 : INFO : EPOCH 2 - PROGRESS: at 10.55% examples, 415013 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:15:57,597 : INFO : EPOCH 2 - PROGRESS: at 21.61% examples, 425098 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:15:58,603 : INFO : EPOCH 2 - PROGRESS: at 32.44% examples, 423699 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:15:59,616 : INFO : EPOCH 2 - PROGRESS: at 43.78% examples, 427531 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:16:00,655 : INFO : EPOCH 2 - PROGRESS: at 54.79% examples, 425298 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:16:01,651 : INFO : EPOCH 2 - PROGRESS: at 65.75% examples, 426312 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:16:02,657 : INFO : EPOCH 2 - PROGRESS: at 76.66% examples, 426679 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:16:03,666 : INFO : EPOCH 2 - PROGRESS: at 87.48% examples, 426696 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:16:04,667 : INFO : EPOCH 2 - PROGRESS: at 98.77% examples, 427844 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:16:04,722 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2019-06-06 02:16:04,730 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2019-06-06 02:16:04,743 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2019-06-06 02:16:04,751 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-06-06 02:16:04,752 : INFO : EPOCH - 2 : training on 5920724 raw words (3933412 effective words) took 9.2s, 429040 effective words/s\n",
            "2019-06-06 02:16:05,813 : INFO : EPOCH 3 - PROGRESS: at 10.71% examples, 403540 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:16:06,828 : INFO : EPOCH 3 - PROGRESS: at 21.95% examples, 419890 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:16:07,843 : INFO : EPOCH 3 - PROGRESS: at 32.76% examples, 419402 words/s, in_qsize 8, out_qsize 1\n",
            "2019-06-06 02:16:08,848 : INFO : EPOCH 3 - PROGRESS: at 43.95% examples, 423435 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:16:09,852 : INFO : EPOCH 3 - PROGRESS: at 54.97% examples, 424708 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:16:10,852 : INFO : EPOCH 3 - PROGRESS: at 65.75% examples, 424771 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:16:11,867 : INFO : EPOCH 3 - PROGRESS: at 77.00% examples, 426643 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:16:12,878 : INFO : EPOCH 3 - PROGRESS: at 87.47% examples, 424886 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:16:13,884 : INFO : EPOCH 3 - PROGRESS: at 98.77% examples, 426053 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:16:13,941 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2019-06-06 02:16:13,951 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2019-06-06 02:16:13,971 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2019-06-06 02:16:13,977 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-06-06 02:16:13,978 : INFO : EPOCH - 3 : training on 5920724 raw words (3933779 effective words) took 9.2s, 426823 effective words/s\n",
            "2019-06-06 02:16:14,994 : INFO : EPOCH 4 - PROGRESS: at 10.71% examples, 421947 words/s, in_qsize 8, out_qsize 1\n",
            "2019-06-06 02:16:16,005 : INFO : EPOCH 4 - PROGRESS: at 21.45% examples, 420339 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:16:17,043 : INFO : EPOCH 4 - PROGRESS: at 32.76% examples, 423055 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:16:18,064 : INFO : EPOCH 4 - PROGRESS: at 44.10% examples, 426267 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:16:19,067 : INFO : EPOCH 4 - PROGRESS: at 55.28% examples, 428252 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:16:20,097 : INFO : EPOCH 4 - PROGRESS: at 66.59% examples, 428933 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:16:21,104 : INFO : EPOCH 4 - PROGRESS: at 77.65% examples, 429764 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:16:22,126 : INFO : EPOCH 4 - PROGRESS: at 88.82% examples, 430430 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:16:23,091 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2019-06-06 02:16:23,096 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2019-06-06 02:16:23,098 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2019-06-06 02:16:23,099 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-06-06 02:16:23,103 : INFO : EPOCH - 4 : training on 5920724 raw words (3934080 effective words) took 9.1s, 431598 effective words/s\n",
            "2019-06-06 02:16:24,118 : INFO : EPOCH 5 - PROGRESS: at 10.71% examples, 423774 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:16:25,124 : INFO : EPOCH 5 - PROGRESS: at 21.64% examples, 425588 words/s, in_qsize 6, out_qsize 1\n",
            "2019-06-06 02:16:26,128 : INFO : EPOCH 5 - PROGRESS: at 32.60% examples, 427180 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:16:27,145 : INFO : EPOCH 5 - PROGRESS: at 43.78% examples, 428214 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:16:28,167 : INFO : EPOCH 5 - PROGRESS: at 55.12% examples, 429527 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:16:29,178 : INFO : EPOCH 5 - PROGRESS: at 66.26% examples, 430095 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:16:30,181 : INFO : EPOCH 5 - PROGRESS: at 77.17% examples, 430142 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:16:31,192 : INFO : EPOCH 5 - PROGRESS: at 88.31% examples, 431210 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:16:32,184 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2019-06-06 02:16:32,198 : INFO : EPOCH 5 - PROGRESS: at 99.65% examples, 431646 words/s, in_qsize 2, out_qsize 1\n",
            "2019-06-06 02:16:32,199 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2019-06-06 02:16:32,209 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2019-06-06 02:16:32,214 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-06-06 02:16:32,215 : INFO : EPOCH - 5 : training on 5920724 raw words (3933706 effective words) took 9.1s, 432312 effective words/s\n",
            "2019-06-06 02:16:32,216 : INFO : training on a 29603620 raw words (19669013 effective words) took 45.8s, 428987 effective words/s\n",
            "2019-06-06 02:16:32,217 : INFO : precomputing L2-norms of word weight vectors\n",
            "2019-06-06 02:16:32,267 : INFO : saving Word2Vec object under 300features_40minwords_10context, separately None\n",
            "2019-06-06 02:16:32,268 : INFO : not storing attribute vectors_norm\n",
            "2019-06-06 02:16:32,270 : INFO : not storing attribute cum_table\n",
            "2019-06-06 02:16:32,271 : WARNING : this function is deprecated, use smart_open.open instead\n",
            "2019-06-06 02:16:32,510 : INFO : saved 300features_40minwords_10context\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRHjnKP3O2-1",
        "colab_type": "code",
        "outputId": "94622392-c3b1-44af-9395-b5f3fd0d9149",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "source": [
        "model.doesnt_match(\"man woman child kitchen\".split())\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `doesnt_match` (Method will be removed in 4.0.0, use self.wv.doesnt_match() instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/models/keyedvectors.py:895: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
            "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'kitchen'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HrctvxdO26f",
        "colab_type": "code",
        "outputId": "d806b3e5-8cdc-4553-980b-bec741f50c0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        }
      },
      "source": [
        "\n",
        "model.most_similar(\"man\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('woman', 0.6654655933380127),\n",
              " ('doctor', 0.6339104175567627),\n",
              " ('boy', 0.6286064982414246),\n",
              " ('journalist', 0.594315767288208),\n",
              " ('businessman', 0.5905495285987854),\n",
              " ('lady', 0.5899538397789001),\n",
              " ('guy', 0.5748952627182007),\n",
              " ('soldier', 0.5712598562240601),\n",
              " ('scientist', 0.5709996223449707),\n",
              " ('priest', 0.5663358569145203)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jujvNDbPKve",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Vector Averaging\n",
        "import numpy as np # Make sure that numpy is imported\n",
        "def makeFeatureVec(words, model, num_features):\n",
        "# Function to average all of the word vectors in a given\n",
        "# paragraph\n",
        "#\n",
        "# Pre-initialize an empty numpy array (for speed)\n",
        "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
        "#\n",
        "    nwords = 0.\n",
        "#\n",
        "# Index2word is a list that contains the names of the words in\n",
        "# the model's vocabulary. Convert it to a set, for speed\n",
        "    index2word_set = set(model.wv.index2word)\n",
        "#\n",
        "# Loop over each word in the review and, if it is in the model's\n",
        "# vocaublary, add its feature vector to the total\n",
        "    for word in words:\n",
        "        if word in index2word_set:\n",
        "            nwords = nwords + 1.\n",
        "            featureVec = np.add(featureVec,model[word])\n",
        "#\n",
        "# Divide the result by the number of words to get the average\n",
        "    featureVec = np.divide(featureVec,nwords)\n",
        "    return featureVec\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c058MlCUPK3n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getAvgFeatureVecs(reviews, model, num_features):\n",
        "# Given a set of reviews (each one a list of words), calculate\n",
        "# the average feature vector for each one and return a 2D numpy array\n",
        "#\n",
        "# Initialize a counter\n",
        "    counter = 0\n",
        "#\n",
        "# Preallocate a 2D numpy array, for speed\n",
        "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
        "# Loop through the reviews\n",
        "    for review in reviews:\n",
        "        if counter%1000 == 0:\n",
        "            print (\"Review %d of %d\" % (counter, len(reviews)))\n",
        "        reviewFeatureVecs[counter] = makeFeatureVec(review, model,num_features)\n",
        "#\n",
        "# Increment the counter\n",
        "        counter = counter + 1\n",
        "    return reviewFeatureVecs\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjB4MMtqPLCN",
        "colab_type": "code",
        "outputId": "cfee0286-78b8-4ef6-9f2a-f8fe52a82d79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "clean_train_reviews = []\n",
        "for review in train1:\n",
        "    clean_train_reviews.append( review_to_wordlist( review,         remove_stopwords=True ))\n",
        "    \n",
        "trainDataVecs = getAvgFeatureVecs( clean_train_reviews, model, num_features )\n",
        "print (\"Creating average feature vecs for test reviews\")\n",
        "clean_test_reviews = []\n",
        "for review in test1:\n",
        "    clean_test_reviews.append( review_to_wordlist( review,         remove_stopwords=True ))\n",
        "\n",
        "\n",
        "# In[52]:"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-94-ad58fd9a1297>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mclean_train_reviews\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mreview\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mclean_train_reviews\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mreview_to_wordlist\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mreview\u001b[0m\u001b[0;34m,\u001b[0m         \u001b[0mremove_stopwords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrainDataVecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetAvgFeatureVecs\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mclean_train_reviews\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_features\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train1' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajbzoLmrPK_I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def expand(lll):\n",
        "  s = []\n",
        "  for ll in lll:\n",
        "    for l in ll:\n",
        "      s.append(l)\n",
        "  return s   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DHiVhllPK0z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "l = [[1],[2,3],[4],[5],[5,5],[5],[5]]\n",
        "\n",
        "q = expand(l)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EiQPe8kYd8tc",
        "colab_type": "code",
        "outputId": "ff5754d4-8944-433b-ca6c-0a6f87c2e0e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(q)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 2, 3, 4, 5, 5, 5, 5, 5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDoM4k_bPKyo",
        "colab_type": "code",
        "outputId": "254610f6-0e82-4cec-e069-04c198483c36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "l = [[1],[2,3],[4],[5],[5,5],[5],[5]]\n",
        "q = expand(l)\n",
        "\n",
        "collections.Counter(q)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({1: 1, 2: 1, 3: 1, 4: 1, 5: 5})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3difQDQudksi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}