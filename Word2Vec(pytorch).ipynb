{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled3.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7yYE7X7_mgS",
        "colab_type": "code",
        "outputId": "a8930859-f6c5-489b-eec3-de75ff646f16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "import keras, os, pickle, re, sklearn, string, tensorflow\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from keras.utils import to_categorical\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "from keras.layers import Embedding\n",
        "from keras.optimizers import Adadelta\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "print('Keras version: \\t\\t%s' % keras.__version__)\n",
        "print('Scikit version: \\t%s' % sklearn.__version__)\n",
        "print('TensorFlow version: \\t%s' % tensorflow.__version__)\n"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Keras version: \t\t2.2.4\n",
            "Scikit version: \t0.21.2\n",
            "TensorFlow version: \t1.13.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QK926j1NHzwm",
        "colab_type": "code",
        "outputId": "2c219c67-574f-41a6-9c16-a0c2dad7c818",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# Download the punkt tokenizer for sentence splitting\n",
        "import nltk.data\n",
        "nltk.download('punkt')   \n",
        "\n",
        "# Load the punkt tokenizer\n",
        "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "\n",
        "# Define a function to split a review into parsed sentences\n",
        "def review_to_sentences( review, tokenizer, remove_stopwords=False ):\n",
        "    # Function to split a review into parsed sentences. Returns a \n",
        "    # list of sentences, where each sentence is a list of words\n",
        "    #\n",
        "    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
        "    raw_sentences = tokenizer.tokenize(review.strip())\n",
        "    #\n",
        "    # 2. Loop over each sentence\n",
        "    sentences = []\n",
        "    for raw_sentence in raw_sentences:\n",
        "        # If a sentence is empty, skip it\n",
        "        if len(raw_sentence) > 0:\n",
        "            # Otherwise, call review_to_wordlist to get a list of words\n",
        "            sentences.append( review_to_wordlist( raw_sentence,remove_stopwords ))\n",
        "    #\n",
        "    # Return the list of sentences (each sentence is a list of words,\n",
        "    # so this returns a list of lists\n",
        "    return sentences\n"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6i2vWZB-JmoV",
        "colab_type": "code",
        "outputId": "f2308938-48c1-4a67-a79d-398fcd810774",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')# Download text data sets, including stop words\n",
        "from nltk.corpus import stopwords # Import the stop word list\n",
        "import re\n",
        "from bs4 import BeautifulSoup"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhSC5kt8MJ8z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import various modules for string cleaning\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "def review_to_wordlist( review, remove_stopwords=False ):\n",
        "# Function to convert a document to a sequence of words,\n",
        "# optionally removing stop words. Returns a list of words.\n",
        "#\n",
        "# 1. Remove HTML\n",
        "    review_text = BeautifulSoup(review).get_text()\n",
        "#\n",
        "# 2. Remove non-letters\n",
        "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
        "#\n",
        "# 3. Convert words to lower case and split them\n",
        "    words = review_text.lower().split()\n",
        "#\n",
        "# 4. Optionally remove stop words (false by default)\n",
        "    if remove_stopwords:\n",
        "        stops = set(stopwords.words(\"english\"))\n",
        "        words = [w for w in words if not w in stops]\n",
        "#\n",
        "# 5. Return a list of words\n",
        "    return(words)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4Bi8oMQNTg0",
        "colab_type": "code",
        "outputId": "0ff0c9f7-36ee-4ba0-89ce-82e0fc860b80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# Download the punkt tokenizer for sentence splitting\n",
        "import nltk.data\n",
        "nltk.download('punkt')\n",
        "# Load the punkt tokenizer\n",
        "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "# Define a function to split a review into parsed sentences\n",
        "def review_to_sentences( review, tokenizer, remove_stopwords=False ):\n",
        "# Function to split a review into parsed sentences. Returns a\n",
        "# list of sentences, where each sentence is a list of words\n",
        "#\n",
        "# 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
        "    raw_sentences = tokenizer.tokenize(review.strip())\n",
        "#\n",
        "# 2. Loop over each sentence\n",
        "    sentences = []\n",
        "    for raw_sentence in raw_sentences:\n",
        "# If a sentence is empty, skip it\n",
        "        if len(raw_sentence) > 0:\n",
        "# Otherwise, call review_to_wordlist to get a list of words\n",
        "            sentences.append( review_to_wordlist( raw_sentence, remove_stopwords ))\n",
        "#\n",
        "# Return the list of sentences (each sentence is a list of words,\n",
        "# so this returns a list of lists\n",
        "    return sentences\n"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLymiuSicQGL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "path = '/content/gdrive/My Drive/data/labeledTrainData.tsv'\n",
        "\n",
        "data1 = pd.read_csv(path, header=0, delimiter=\"\\t\", quoting=3)\n",
        "data = data1['review']\n",
        "target = data1['sentiment']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kEv3xQleARV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybSrRiltdYO2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn import datasets, linear_model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# create training and testing vars\n",
        "train, test, ytrain, ytest = train_test_split(data, target, test_size=0.2)\n",
        "#print X_train.shape, y_train.shape\n",
        "#print X_test.shape, y_test.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8IIPXlQXy4Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xtrain = train.values.reshape(-1,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tITpCjRKhGYJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "313fbf31-1571-4308-e769-853da5b3d580"
      },
      "source": [
        "xtrain"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([['\"What a load of Leftist Hollywood bilge. This movie glorifies mutiny as brave and noble if it be for pacifist principles. The fairytale ends with the pacifist character, played by Danzel Washington, actually getting promoted for his treason. What is it with these Hollywood tools? Is this still payback for McCarthyism?<br /><br />If I sound cynical it\\'s because I am fed up with movies hawking a political agenda. The military brass in this movie are portrayed as, what else? Gung-ho war mongers. Sound familiar? Ever see a movie where the CIA or any government agency is not evil? Think about it. Yet again, Crimson Tide stresses the point. The Hackman character, U-boat captain Ramsey, comes across like a raving lunatic, until the very end when, of course he comes to his senses, does a complete 360, renounces his blood lust, suggests a promotion for the treasonous Ron Hunter, and repents by retiring from the service. A guy mutinies, takes command of your boat, puts the U.S at grave risk of receiving a nuclear first-strike, and you promote him???? What hogwash!\"'],\n",
              "       ['\"Jesus Christ, I can\\'t believe I\\'ve wasted my time watching this movie. I only watched because I have such a crush on Jordan Ladd. But watching this film almost put me off her. This is absolutely awful! I could have been watching Survivor Series 93 over this.<br /><br />The lead guy in this was so bland and generic. I would love it if the great Mistuharu Misawa Tiger Drove \\'91\\'d his ass through a glass window. I was enraging every time he was saying \\\\\"lake\\\\\" and \\\\\"cabin\\\\\". I\\'d kick his ass.<br /><br />Jordan Ladd, on the other hand, was absolutely wonderful. A true angel. But she couldn\\'t even save this utter joke of a film. Sadly, she couldn\\'t even act like she was off her nut when she took that truth drug. It looked hilarious.<br /><br />I also loved the bit where Jordan accidentally spilled yogurt on her. It reminded me of a time where...nevermind.<br /><br />Anayways, do watch this film because of it\\'s awfulness.\"'],\n",
              "       ['\"*** REVIEW MAY CONTAIN SOME SPOILERS *** I\\'ll make this review short and sweet. I bought this movie from Best Buy because it sounded interested and had some top actors in it like Kevin Spacey and Morgan Freeman. How bad could it be, right? Well, it\\'s pretty bad. Justin Timberlake plays Pollack, a wannabe journalist who stumbles across a case that may lead to corrupt cops at Edison\\'s Police Force. LL Cool J is Deed, a cop within the force on a special force team called F.R.A.T. (First Response Assault Tactics). He\\'s teamed with an \\\\\"on-the-edge\\\\\" bad cop named Lazerov (Dylan McDermott). In the opening scene we see Lazerov & Deed taking on some bank robbers, but at night they are busting a couple of guys doing drugs. I don\\'t want to give to much away, but things turn bad for the guys doing the drugs. Pollack, who works for Ashford (Morgan Freeman) goes to a trial involving Deeds & Lazerov. He suspect foul play and with the help of Ashford, does some investigate that turns ugly. Wallace (Kevin Spacey) who is all within the F.R.A.T. team joins with Ashford to try to bring the corrupt cops to justice.<br /><br />You can tell from the beginning that Freeman and Spacey\\'s performance are pretty lackluster. The only person that give a all out performance is Dylan McDermott. He is a complete nut case in this movie and made a believer out of me. LL Cool J is terrible in this film. He says every line the same way and shows pretty much the same emotion. He was much better in movies like Deep Blue Sea & Any Given Sunday. The film starts off with some nice action but then drags it feet through the rest of the film. The ending is far from satisfying.<br /><br />Don\\'t waste your time with this film. I\\'m putting it on Ebay this weekend.\"'],\n",
              "       ...,\n",
              "       ['\"Coinciding with the start of the baby boom, the years after World War II saw an unprecedented exodus of Americans moving out of their city apartments into the suburbs where they can fulfill their dreams of owning their own homes. Directed by H.C. Potter and co-written by Norman Panama and Melvin Frank (\\\\\"White Christmas\\\\\"), this lightweight but surprisingly observant 1948 screwball comedy captures the feeling of that period very well. Of course, it helps to have a trio of expert farceurs \\x96 Cary Grant, Myrna Loy and an especially acerbic Melvyn Douglas \\x96 head the proceedings with their natural likability at odds with the escalating frustrations of home ownership. Even though the film is sixty years old now, there is a timeless quality to the Blandings\\' dream and the barriers they face in achieving it. Obviously, Hollywood thinks so since it\\'s been remade at least twice - first as a very physical Tom Hanks comedy, 1986\\'s \\\\\"The Money Pit\\\\\", and again last year with Ice Cube\\'s \\\\\"Are We Done Yet?\\\\\". One look at HGTV\\'s programming schedule will show you how the situations explored here still resonate today.<br /><br />The plot begins with ad man Jim Blandings, his wife Muriel and their two daughters cramped into a two bedroom-one bath Manhattan apartment. Rather than pursue Muriel\\'s idea to renovate the apartment for $7,000, Jim sees a photo of a Connecticut house in a magazine and realizes this is where they need to move. With the help of an opportunistic real estate agent and against the advice of their attorney and family friend Bill Cole, the Blandings decide to buy a ramshackle house badly in need of repair. However, the foundation sags so badly that the house needs to be torn down in favor of a new one. This sparks the Blandings to push the architect to design a house so excessive that the second floor is twice as big as the first. Costs rise with each new complication, tempers flare, and even a romantic triangle is imagined among, Jim, Muriel and Bill. Priorities finally sort themselves out but not before some funny slapstick scenes and clever dialogue that tweaks the not-so-blissful ignorance of the new homeowners.<br /><br />With his double takes and flawless line delivery, Grant is infallible in this type of farce, and Jim Blandings epitomizes his more domesticated mid-career characters. In a role originally meant for Irene Dunne, Myrna Loy shows why she was Hollywood\\'s perfect wife. She doesn\\'t get many of the funnier lines, but she combines her special blend of flightiness and sauciness to make Muriel an appealing character on her own. Watch her deftly maneuver the overly agreeable house painter with her absurdly idiosyncratic color palette. As avuncular, pipe-smoking Bill (\\\\\"Cole\\x85Bill Cole\\\\\"), Melvyn Douglas shows his natural, easy-going élan as Grant\\'s foil. Smaller roles are filled expertly with particularly memorable turns by Harry Shannon as the laconic well-digger Mr. Tesander, Lurene Tuttle as Jim\\'s officious assistant Mary, and Louise Beavers as the Blandings\\' lovable maid Gussie. The 2004 DVD provides some intriguing vintage material including two radio versions of the movie - the first a 1949 version that did end up pairing Grant and Dunne and then a second 1950 version coupling Grant with his then-wife, actress Betsy Drake. A most appropriate 1949 cartoon, \\\\\"The House of Tomorrow\\\\\", is also included giving us a comical tour of a futuristic dream house. The original theatrical trailers for ten of Grant\\'s film classics complete the extras.\"'],\n",
              "       ['\"I never bothered to see this movie in theaters although I remember hearing the name over and over. I finally watched it this week and what a delight. For some reason, I was expecting it to not be very good so I was completely surprised when I sat down and stuck with it and then found myself completely pulled in. I read a lot of the other user comments and it impressed me how much people talk about her fighting in the ring, but what was wonderful about Diana is that she\\'s a true fighter in life. All she needs to do is find her place where she be who she is and the ring helps her to get there. A very intelligent story and I\\'m amazed that this is the first time up for Michelle Rodriguez - what an excellent job she did. Adrian and her coach were also quite good.<br /><br />This film is a little rough around the edges, but it doesn\\'t matter in the slightest. The story, the will, and the performances completely outweigh any flaws (that usually come with indie filmmaking anyway). A compelling portrayal of a girl finding herself and triumphing over her circumstances and a K.O. for Michelle Rodriguez!\"'],\n",
              "       ['\"First off, anyone who thinks this sequel to William Friedkin\\'s \\\\\"The French Connection\\\\\", is superior is most definitely completely insane or moronic or both. The problem with reviewing this film is that, a.) it\\'s a sequel to a brilliant movie, which always makes watching it objectively difficult, and b.) it\\'s directed by John Frankenhimer, one of the best American directors ever, so I wanted to like it. William Friendkin was the perfect person to direct a film about drug traffic in decaying new York city, because of his documentary-like approach to the action and story, Frankenhimer on the other hand is one of the most stylish directors ever, i.e. \\\\\"The Manchurian Candidate\\\\\" and \\\\\"Seconds\\\\\", and with his \\\\\"French Connection 2\\\\\" it feels like someone trying to be gritty and not having the true understanding to pull it off. That fact that Frankenhimer was chosen to direct the sequel by Gene Hackman himself really tells a lot about Hackman\\'s understanding about the original film too. It\\'s well known Hackman hated Friedkin on the set and vowed to never work with him again, it\\'s also known he envisioned the character to be more one dimensional, loosing weight and trying to play him like a straight character. it shows you Hackman, despite being a great actor, had no idea who to make the movie and the story great. The plot point of Doyle becoming an addict is interesting, but doesn\\'t warrant the rest of the film. An unfortunate low point in Frankenhimer\\'s filmography.\"']],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6DIOdTyO3IW",
        "colab_type": "code",
        "outputId": "821d3e12-cc30-47d8-f24d-c87d57986db7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "'''\n",
        "from google.colab import drive\n",
        "import glob\n",
        "import os\n",
        "'''\n",
        "'''\n",
        "drive.mount('/content/gdrive/')\n",
        "!ls \"/content/gdrive/My Drive\"\n",
        "download_path = os.path.expanduser(\"/content/gdrive/My Drive\")\n",
        "try:\n",
        "  os.makedirs(download_path)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "  \n",
        "'''\n",
        "'''\n",
        "drive.mount('/content/gdrive/')\n",
        "\n",
        "\n",
        "def read_files1(path):\n",
        "    \n",
        "    #drive.mount('/content/gdrive/')\n",
        "\n",
        "    !mkdir drive\n",
        "    !google-drive-ocamlfuse drive\n",
        "    !ls \"gdrive/My Drive/aclImdb\"\n",
        "\n",
        "    print(path)\n",
        "    !ls path\n",
        "    #path = glob.glob('/gdrive/aclImdb/train/pos/')\n",
        "    print(path)\n",
        "    documents = list()\n",
        "    print(\"11111\")\n",
        "    i = 0\n",
        "    for filename in os.listdir(path):\n",
        "      with open('%s/%s' % (path, filename),encoding='utf-8') as f:\n",
        "        print(\"2222\")\n",
        "        i = i+1\n",
        "        print(i)\n",
        "        doc = f.read()\n",
        "        documents.append(doc)\n",
        "        if i == 4:\n",
        "          break\n",
        "        \n",
        "    if os.path.isfile(path):\n",
        "        with open(path, encoding='iso-8859-1') as f:\n",
        "            doc = f.readlines()\n",
        "            for line in doc:\n",
        "                documents.append(line)\n",
        "    \n",
        "    return documents\n",
        "'''"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ndrive.mount(\\'/content/gdrive/\\')\\n\\n\\ndef read_files1(path):\\n    \\n    #drive.mount(\\'/content/gdrive/\\')\\n\\n    !mkdir drive\\n    !google-drive-ocamlfuse drive\\n    !ls \"gdrive/My Drive/aclImdb\"\\n\\n    print(path)\\n    !ls path\\n    #path = glob.glob(\\'/gdrive/aclImdb/train/pos/\\')\\n    print(path)\\n    documents = list()\\n    print(\"11111\")\\n    i = 0\\n    for filename in os.listdir(path):\\n      with open(\\'%s/%s\\' % (path, filename),encoding=\\'utf-8\\') as f:\\n        print(\"2222\")\\n        i = i+1\\n        print(i)\\n        doc = f.read()\\n        documents.append(doc)\\n        if i == 4:\\n          break\\n        \\n    if os.path.isfile(path):\\n        with open(path, encoding=\\'iso-8859-1\\') as f:\\n            doc = f.readlines()\\n            for line in doc:\\n                documents.append(line)\\n    \\n    return documents\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43DtcD11O3Gi",
        "colab_type": "code",
        "outputId": "2cf7dad4-d28a-4b89-8342-cd0911dfbb14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "'''\n",
        "pos_train = read_files1('/content/gdrive/My Drive/aclImdb/train/pos')\n",
        "neg_train = read_files1('/content/gdrive/My Drive/aclImdb/train/neg')\n",
        "pos_test = read_files1('/content/gdrive/My Drive/aclImdb/test/pos')\n",
        "neg_test = read_files1('/content/gdrive/My Drive/aclImdb/test/neg')\n",
        "\n",
        "\n",
        "train1 = pos_train + neg_train\n",
        "test1 = pos_test + neg_test\n",
        "\n",
        "#docs = negative_docs + positive_docs\n",
        "l_train = [1 for _ in range(len(pos_train))] + [0 for _ in range(len(neg_train))]\n",
        "l_test = [1 for _ in range(len(pos_train))] + [0 for _ in range(len(neg_test))]\n",
        "train = np.column_stack((train1,l_train))\n",
        "test = np.column_stack((test1,l_test))\n",
        "'''"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\npos_train = read_files1('/content/gdrive/My Drive/aclImdb/train/pos')\\nneg_train = read_files1('/content/gdrive/My Drive/aclImdb/train/neg')\\npos_test = read_files1('/content/gdrive/My Drive/aclImdb/test/pos')\\nneg_test = read_files1('/content/gdrive/My Drive/aclImdb/test/neg')\\n\\n\\ntrain1 = pos_train + neg_train\\ntest1 = pos_test + neg_test\\n\\n#docs = negative_docs + positive_docs\\nl_train = [1 for _ in range(len(pos_train))] + [0 for _ in range(len(neg_train))]\\nl_test = [1 for _ in range(len(pos_train))] + [0 for _ in range(len(neg_test))]\\ntrain = np.column_stack((train1,l_train))\\ntest = np.column_stack((test1,l_test))\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJLEw5dyZt9p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPA-qWaOWT2p",
        "colab_type": "code",
        "outputId": "9194347a-c872-4256-f110-8b567c38ee50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "sentences = []  # Initialize an empty list of sentences\n",
        "\n",
        "print (\"Parsing sentences from training set\")\n",
        "for review in data1[\"review\"]:\n",
        "    sentences += review_to_sentences(review, tokenizer)\n"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parsing sentences from training set\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:273: UserWarning: \"b'.'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
            "  ' Beautiful Soup.' % markup)\n",
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:336: UserWarning: \"http://www.happierabroad.com\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAJzUW4ki-C_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "outputId": "fe46f7a5-207b-47c9-dd16-77881aed926e"
      },
      "source": [
        "sentences[1]"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['maybe',\n",
              " 'i',\n",
              " 'just',\n",
              " 'want',\n",
              " 'to',\n",
              " 'get',\n",
              " 'a',\n",
              " 'certain',\n",
              " 'insight',\n",
              " 'into',\n",
              " 'this',\n",
              " 'guy',\n",
              " 'who',\n",
              " 'i',\n",
              " 'thought',\n",
              " 'was',\n",
              " 'really',\n",
              " 'cool',\n",
              " 'in',\n",
              " 'the',\n",
              " 'eighties',\n",
              " 'just',\n",
              " 'to',\n",
              " 'maybe',\n",
              " 'make',\n",
              " 'up',\n",
              " 'my',\n",
              " 'mind',\n",
              " 'whether',\n",
              " 'he',\n",
              " 'is',\n",
              " 'guilty',\n",
              " 'or',\n",
              " 'innocent']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49_bGNsNO3Ek",
        "colab_type": "code",
        "outputId": "f59d4a66-c963-49e9-9cf2-475465c685bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571
        }
      },
      "source": [
        ""
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parsing sentences from training set\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-79-8b5b5d1667c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mreview\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_reviews\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview\u001b[0m \u001b[0;34m!=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreview_to_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m '''\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Parsing sentences from unlabeled set\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    866\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    869\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, series, key)\u001b[0m\n\u001b[1;32m   4373\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4374\u001b[0m             return self._engine.get_value(s, k,\n\u001b[0;32m-> 4375\u001b[0;31m                                           tz=getattr(series.dtype, 'tz', None))\n\u001b[0m\u001b[1;32m   4376\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4377\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mholds_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_boolean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 3"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnFRlwSpO3CY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2028
        },
        "outputId": "80ad7066-2c2c-49ae-82ad-af07547307e5"
      },
      "source": [
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',    level=logging.INFO)\n",
        "\n",
        "def myhashfxn(obj):\n",
        "    return hash(obj) % (2 ** 32)\n",
        "\n",
        "# Set values for various parameters\n",
        "num_features = 400    # Word vector dimensionality                      \n",
        "min_word_count = 60   # Minimum word count                        \n",
        "num_workers = 4       # Number of threads to run in parallel\n",
        "context = 10          # Context window size                                                                                    \n",
        "downsampling = 1e-3   # Downsample setting for frequent words\n",
        "\n",
        "\n",
        "#**********************************************************\n",
        "\n",
        "\n",
        "\n",
        "# Initialize and train the model (this will take some time)\n",
        "from gensim.models import word2vec\n",
        "#model = word2vec.Word2Vec(hashfxn=myhashfxn)\n",
        "\n",
        "print (\"Training model...\")\n",
        "model = word2vec.Word2Vec(sentences, workers=num_workers,             size=num_features, min_count = min_word_count,             window = context, sample = downsampling,hashfxn=myhashfxn)\n",
        "\n",
        "# If you don't plan to train the model any further, calling \n",
        "# init_sims will make the model much more memory-efficient.\n",
        "model.init_sims(replace=True)\n",
        "\n",
        "# It can be helpful to create a meaningful model name and \n",
        "# save the model for later use. You can load it later using Word2Vec.load()\n",
        "model_name = \"300features_40minwords_10context\"\n",
        "model.save(model_name)\n"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-06-06 02:15:44,719 : INFO : 'pattern' package not found; tag filters are not available for English\n",
            "2019-06-06 02:15:44,729 : INFO : collecting all words and their counts\n",
            "2019-06-06 02:15:44,730 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2019-06-06 02:15:44,786 : INFO : PROGRESS: at sentence #10000, processed 225803 words, keeping 17776 word types\n",
            "2019-06-06 02:15:44,845 : INFO : PROGRESS: at sentence #20000, processed 451892 words, keeping 24948 word types\n",
            "2019-06-06 02:15:44,901 : INFO : PROGRESS: at sentence #30000, processed 671315 words, keeping 30034 word types\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training model...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-06-06 02:15:44,963 : INFO : PROGRESS: at sentence #40000, processed 897815 words, keeping 34348 word types\n",
            "2019-06-06 02:15:45,019 : INFO : PROGRESS: at sentence #50000, processed 1116963 words, keeping 37761 word types\n",
            "2019-06-06 02:15:45,073 : INFO : PROGRESS: at sentence #60000, processed 1338404 words, keeping 40723 word types\n",
            "2019-06-06 02:15:45,127 : INFO : PROGRESS: at sentence #70000, processed 1561580 words, keeping 43333 word types\n",
            "2019-06-06 02:15:45,181 : INFO : PROGRESS: at sentence #80000, processed 1780887 words, keeping 45714 word types\n",
            "2019-06-06 02:15:45,235 : INFO : PROGRESS: at sentence #90000, processed 2004996 words, keeping 48135 word types\n",
            "2019-06-06 02:15:45,289 : INFO : PROGRESS: at sentence #100000, processed 2226966 words, keeping 50207 word types\n",
            "2019-06-06 02:15:45,342 : INFO : PROGRESS: at sentence #110000, processed 2446580 words, keeping 52081 word types\n",
            "2019-06-06 02:15:45,397 : INFO : PROGRESS: at sentence #120000, processed 2668775 words, keeping 54119 word types\n",
            "2019-06-06 02:15:45,452 : INFO : PROGRESS: at sentence #130000, processed 2894303 words, keeping 55847 word types\n",
            "2019-06-06 02:15:45,504 : INFO : PROGRESS: at sentence #140000, processed 3107005 words, keeping 57346 word types\n",
            "2019-06-06 02:15:45,558 : INFO : PROGRESS: at sentence #150000, processed 3332627 words, keeping 59055 word types\n",
            "2019-06-06 02:15:45,613 : INFO : PROGRESS: at sentence #160000, processed 3555315 words, keeping 60617 word types\n",
            "2019-06-06 02:15:45,666 : INFO : PROGRESS: at sentence #170000, processed 3778655 words, keeping 62077 word types\n",
            "2019-06-06 02:15:45,718 : INFO : PROGRESS: at sentence #180000, processed 3999236 words, keeping 63496 word types\n",
            "2019-06-06 02:15:45,771 : INFO : PROGRESS: at sentence #190000, processed 4224449 words, keeping 64794 word types\n",
            "2019-06-06 02:15:45,828 : INFO : PROGRESS: at sentence #200000, processed 4448603 words, keeping 66087 word types\n",
            "2019-06-06 02:15:45,881 : INFO : PROGRESS: at sentence #210000, processed 4669967 words, keeping 67390 word types\n",
            "2019-06-06 02:15:45,934 : INFO : PROGRESS: at sentence #220000, processed 4894968 words, keeping 68697 word types\n",
            "2019-06-06 02:15:45,988 : INFO : PROGRESS: at sentence #230000, processed 5117545 words, keeping 69958 word types\n",
            "2019-06-06 02:15:46,042 : INFO : PROGRESS: at sentence #240000, processed 5345050 words, keeping 71167 word types\n",
            "2019-06-06 02:15:46,097 : INFO : PROGRESS: at sentence #250000, processed 5559165 words, keeping 72351 word types\n",
            "2019-06-06 02:15:46,151 : INFO : PROGRESS: at sentence #260000, processed 5779146 words, keeping 73478 word types\n",
            "2019-06-06 02:15:46,188 : INFO : collected 74218 word types from a corpus of 5920724 raw words and 266551 sentences\n",
            "2019-06-06 02:15:46,189 : INFO : Loading a fresh vocabulary\n",
            "2019-06-06 02:15:46,239 : INFO : effective_min_count=60 retains 6278 unique words (8% of original 74218, drops 67940)\n",
            "2019-06-06 02:15:46,240 : INFO : effective_min_count=60 leaves 5461149 word corpus (92% of original 5920724, drops 459575)\n",
            "2019-06-06 02:15:46,261 : INFO : deleting the raw counts dictionary of 74218 items\n",
            "2019-06-06 02:15:46,264 : INFO : sample=0.001 downsamples 50 most-common words\n",
            "2019-06-06 02:15:46,265 : INFO : downsampling leaves estimated 3933998 word corpus (72.0% of prior 5461149)\n",
            "2019-06-06 02:15:46,282 : INFO : estimated required memory for 6278 words and 400 dimensions: 23228600 bytes\n",
            "2019-06-06 02:15:46,282 : INFO : resetting layer weights\n",
            "2019-06-06 02:15:46,365 : INFO : training model with 4 workers on 6278 vocabulary and 400 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
            "2019-06-06 02:15:47,385 : INFO : EPOCH 1 - PROGRESS: at 10.38% examples, 406628 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:15:48,408 : INFO : EPOCH 1 - PROGRESS: at 21.29% examples, 413575 words/s, in_qsize 7, out_qsize 1\n",
            "2019-06-06 02:15:49,410 : INFO : EPOCH 1 - PROGRESS: at 32.44% examples, 421396 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:15:50,432 : INFO : EPOCH 1 - PROGRESS: at 43.62% examples, 423259 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:15:51,464 : INFO : EPOCH 1 - PROGRESS: at 54.97% examples, 424817 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:15:52,484 : INFO : EPOCH 1 - PROGRESS: at 66.10% examples, 425588 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:15:53,492 : INFO : EPOCH 1 - PROGRESS: at 77.17% examples, 426817 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:15:54,514 : INFO : EPOCH 1 - PROGRESS: at 87.80% examples, 425321 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:15:55,526 : INFO : EPOCH 1 - PROGRESS: at 99.28% examples, 426877 words/s, in_qsize 5, out_qsize 0\n",
            "2019-06-06 02:15:55,543 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2019-06-06 02:15:55,556 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2019-06-06 02:15:55,571 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2019-06-06 02:15:55,572 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-06-06 02:15:55,573 : INFO : EPOCH - 1 : training on 5920724 raw words (3934036 effective words) took 9.2s, 427695 effective words/s\n",
            "2019-06-06 02:15:56,590 : INFO : EPOCH 2 - PROGRESS: at 10.55% examples, 415013 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:15:57,597 : INFO : EPOCH 2 - PROGRESS: at 21.61% examples, 425098 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:15:58,603 : INFO : EPOCH 2 - PROGRESS: at 32.44% examples, 423699 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:15:59,616 : INFO : EPOCH 2 - PROGRESS: at 43.78% examples, 427531 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:16:00,655 : INFO : EPOCH 2 - PROGRESS: at 54.79% examples, 425298 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:16:01,651 : INFO : EPOCH 2 - PROGRESS: at 65.75% examples, 426312 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:16:02,657 : INFO : EPOCH 2 - PROGRESS: at 76.66% examples, 426679 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:16:03,666 : INFO : EPOCH 2 - PROGRESS: at 87.48% examples, 426696 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:16:04,667 : INFO : EPOCH 2 - PROGRESS: at 98.77% examples, 427844 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:16:04,722 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2019-06-06 02:16:04,730 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2019-06-06 02:16:04,743 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2019-06-06 02:16:04,751 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-06-06 02:16:04,752 : INFO : EPOCH - 2 : training on 5920724 raw words (3933412 effective words) took 9.2s, 429040 effective words/s\n",
            "2019-06-06 02:16:05,813 : INFO : EPOCH 3 - PROGRESS: at 10.71% examples, 403540 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:16:06,828 : INFO : EPOCH 3 - PROGRESS: at 21.95% examples, 419890 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:16:07,843 : INFO : EPOCH 3 - PROGRESS: at 32.76% examples, 419402 words/s, in_qsize 8, out_qsize 1\n",
            "2019-06-06 02:16:08,848 : INFO : EPOCH 3 - PROGRESS: at 43.95% examples, 423435 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:16:09,852 : INFO : EPOCH 3 - PROGRESS: at 54.97% examples, 424708 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:16:10,852 : INFO : EPOCH 3 - PROGRESS: at 65.75% examples, 424771 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:16:11,867 : INFO : EPOCH 3 - PROGRESS: at 77.00% examples, 426643 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:16:12,878 : INFO : EPOCH 3 - PROGRESS: at 87.47% examples, 424886 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:16:13,884 : INFO : EPOCH 3 - PROGRESS: at 98.77% examples, 426053 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:16:13,941 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2019-06-06 02:16:13,951 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2019-06-06 02:16:13,971 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2019-06-06 02:16:13,977 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-06-06 02:16:13,978 : INFO : EPOCH - 3 : training on 5920724 raw words (3933779 effective words) took 9.2s, 426823 effective words/s\n",
            "2019-06-06 02:16:14,994 : INFO : EPOCH 4 - PROGRESS: at 10.71% examples, 421947 words/s, in_qsize 8, out_qsize 1\n",
            "2019-06-06 02:16:16,005 : INFO : EPOCH 4 - PROGRESS: at 21.45% examples, 420339 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:16:17,043 : INFO : EPOCH 4 - PROGRESS: at 32.76% examples, 423055 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:16:18,064 : INFO : EPOCH 4 - PROGRESS: at 44.10% examples, 426267 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:16:19,067 : INFO : EPOCH 4 - PROGRESS: at 55.28% examples, 428252 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:16:20,097 : INFO : EPOCH 4 - PROGRESS: at 66.59% examples, 428933 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:16:21,104 : INFO : EPOCH 4 - PROGRESS: at 77.65% examples, 429764 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:16:22,126 : INFO : EPOCH 4 - PROGRESS: at 88.82% examples, 430430 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:16:23,091 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2019-06-06 02:16:23,096 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2019-06-06 02:16:23,098 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2019-06-06 02:16:23,099 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-06-06 02:16:23,103 : INFO : EPOCH - 4 : training on 5920724 raw words (3934080 effective words) took 9.1s, 431598 effective words/s\n",
            "2019-06-06 02:16:24,118 : INFO : EPOCH 5 - PROGRESS: at 10.71% examples, 423774 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:16:25,124 : INFO : EPOCH 5 - PROGRESS: at 21.64% examples, 425588 words/s, in_qsize 6, out_qsize 1\n",
            "2019-06-06 02:16:26,128 : INFO : EPOCH 5 - PROGRESS: at 32.60% examples, 427180 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:16:27,145 : INFO : EPOCH 5 - PROGRESS: at 43.78% examples, 428214 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:16:28,167 : INFO : EPOCH 5 - PROGRESS: at 55.12% examples, 429527 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:16:29,178 : INFO : EPOCH 5 - PROGRESS: at 66.26% examples, 430095 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:16:30,181 : INFO : EPOCH 5 - PROGRESS: at 77.17% examples, 430142 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:16:31,192 : INFO : EPOCH 5 - PROGRESS: at 88.31% examples, 431210 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:16:32,184 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2019-06-06 02:16:32,198 : INFO : EPOCH 5 - PROGRESS: at 99.65% examples, 431646 words/s, in_qsize 2, out_qsize 1\n",
            "2019-06-06 02:16:32,199 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2019-06-06 02:16:32,209 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2019-06-06 02:16:32,214 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-06-06 02:16:32,215 : INFO : EPOCH - 5 : training on 5920724 raw words (3933706 effective words) took 9.1s, 432312 effective words/s\n",
            "2019-06-06 02:16:32,216 : INFO : training on a 29603620 raw words (19669013 effective words) took 45.8s, 428987 effective words/s\n",
            "2019-06-06 02:16:32,217 : INFO : precomputing L2-norms of word weight vectors\n",
            "2019-06-06 02:16:32,267 : INFO : saving Word2Vec object under 300features_40minwords_10context, separately None\n",
            "2019-06-06 02:16:32,268 : INFO : not storing attribute vectors_norm\n",
            "2019-06-06 02:16:32,270 : INFO : not storing attribute cum_table\n",
            "2019-06-06 02:16:32,271 : WARNING : this function is deprecated, use smart_open.open instead\n",
            "2019-06-06 02:16:32,510 : INFO : saved 300features_40minwords_10context\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRHjnKP3O2-1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "outputId": "94622392-c3b1-44af-9395-b5f3fd0d9149"
      },
      "source": [
        "model.doesnt_match(\"man woman child kitchen\".split())\n"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `doesnt_match` (Method will be removed in 4.0.0, use self.wv.doesnt_match() instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/models/keyedvectors.py:895: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
            "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'kitchen'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HrctvxdO26f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "outputId": "d806b3e5-8cdc-4553-980b-bec741f50c0b"
      },
      "source": [
        "\n",
        "model.most_similar(\"man\")\n",
        "\n"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('woman', 0.6654655933380127),\n",
              " ('doctor', 0.6339104175567627),\n",
              " ('boy', 0.6286064982414246),\n",
              " ('journalist', 0.594315767288208),\n",
              " ('businessman', 0.5905495285987854),\n",
              " ('lady', 0.5899538397789001),\n",
              " ('guy', 0.5748952627182007),\n",
              " ('soldier', 0.5712598562240601),\n",
              " ('scientist', 0.5709996223449707),\n",
              " ('priest', 0.5663358569145203)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jujvNDbPKve",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Vector Averaging\n",
        "import numpy as np # Make sure that numpy is imported\n",
        "def makeFeatureVec(words, model, num_features):\n",
        "# Function to average all of the word vectors in a given\n",
        "# paragraph\n",
        "#\n",
        "# Pre-initialize an empty numpy array (for speed)\n",
        "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
        "#\n",
        "    nwords = 0.\n",
        "#\n",
        "# Index2word is a list that contains the names of the words in\n",
        "# the model's vocabulary. Convert it to a set, for speed\n",
        "    index2word_set = set(model.wv.index2word)\n",
        "#\n",
        "# Loop over each word in the review and, if it is in the model's\n",
        "# vocaublary, add its feature vector to the total\n",
        "    for word in words:\n",
        "        if word in index2word_set:\n",
        "            nwords = nwords + 1.\n",
        "            featureVec = np.add(featureVec,model[word])\n",
        "#\n",
        "# Divide the result by the number of words to get the average\n",
        "    featureVec = np.divide(featureVec,nwords)\n",
        "    return featureVec\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c058MlCUPK3n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getAvgFeatureVecs(reviews, model, num_features):\n",
        "# Given a set of reviews (each one a list of words), calculate\n",
        "# the average feature vector for each one and return a 2D numpy array\n",
        "#\n",
        "# Initialize a counter\n",
        "    counter = 0\n",
        "#\n",
        "# Preallocate a 2D numpy array, for speed\n",
        "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
        "# Loop through the reviews\n",
        "    for review in reviews:\n",
        "        if counter%1000 == 0:\n",
        "            print (\"Review %d of %d\" % (counter, len(reviews)))\n",
        "        reviewFeatureVecs[counter] = makeFeatureVec(review, model,num_features)\n",
        "#\n",
        "# Increment the counter\n",
        "        counter = counter + 1\n",
        "    return reviewFeatureVecs\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjB4MMtqPLCN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "cfee0286-78b8-4ef6-9f2a-f8fe52a82d79"
      },
      "source": [
        "clean_train_reviews = []\n",
        "for review in train1:\n",
        "    clean_train_reviews.append( review_to_wordlist( review,         remove_stopwords=True ))\n",
        "    \n",
        "trainDataVecs = getAvgFeatureVecs( clean_train_reviews, model, num_features )\n",
        "print (\"Creating average feature vecs for test reviews\")\n",
        "clean_test_reviews = []\n",
        "for review in test1:\n",
        "    clean_test_reviews.append( review_to_wordlist( review,         remove_stopwords=True ))\n",
        "\n",
        "\n",
        "# In[52]:"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-94-ad58fd9a1297>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mclean_train_reviews\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mreview\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mclean_train_reviews\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mreview_to_wordlist\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mreview\u001b[0m\u001b[0;34m,\u001b[0m         \u001b[0mremove_stopwords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrainDataVecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetAvgFeatureVecs\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mclean_train_reviews\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_features\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train1' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajbzoLmrPK_I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DHiVhllPK0z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDoM4k_bPKyo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}