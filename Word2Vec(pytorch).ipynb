{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled3.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7yYE7X7_mgS",
        "colab_type": "code",
        "outputId": "a8930859-f6c5-489b-eec3-de75ff646f16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "import keras, os, pickle, re, sklearn, string, tensorflow\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from keras.utils import to_categorical\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "from keras.layers import Embedding\n",
        "from keras.optimizers import Adadelta\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "print('Keras version: \\t\\t%s' % keras.__version__)\n",
        "print('Scikit version: \\t%s' % sklearn.__version__)\n",
        "print('TensorFlow version: \\t%s' % tensorflow.__version__)\n"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Keras version: \t\t2.2.4\n",
            "Scikit version: \t0.21.2\n",
            "TensorFlow version: \t1.13.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QK926j1NHzwm",
        "colab_type": "code",
        "outputId": "2c219c67-574f-41a6-9c16-a0c2dad7c818",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# Download the punkt tokenizer for sentence splitting\n",
        "import nltk.data\n",
        "nltk.download('punkt')   \n",
        "\n",
        "# Load the punkt tokenizer\n",
        "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "\n",
        "# Define a function to split a review into parsed sentences\n",
        "def review_to_sentences( review, tokenizer, remove_stopwords=False ):\n",
        "    # Function to split a review into parsed sentences. Returns a \n",
        "    # list of sentences, where each sentence is a list of words\n",
        "    #\n",
        "    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
        "    raw_sentences = tokenizer.tokenize(review.strip())\n",
        "    #\n",
        "    # 2. Loop over each sentence\n",
        "    sentences = []\n",
        "    for raw_sentence in raw_sentences:\n",
        "        # If a sentence is empty, skip it\n",
        "        if len(raw_sentence) > 0:\n",
        "            # Otherwise, call review_to_wordlist to get a list of words\n",
        "            sentences.append( review_to_wordlist( raw_sentence,remove_stopwords ))\n",
        "    #\n",
        "    # Return the list of sentences (each sentence is a list of words,\n",
        "    # so this returns a list of lists\n",
        "    return sentences\n"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6i2vWZB-JmoV",
        "colab_type": "code",
        "outputId": "f2308938-48c1-4a67-a79d-398fcd810774",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')# Download text data sets, including stop words\n",
        "from nltk.corpus import stopwords # Import the stop word list\n",
        "import re\n",
        "from bs4 import BeautifulSoup"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhSC5kt8MJ8z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import various modules for string cleaning\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "def review_to_wordlist( review, remove_stopwords=False ):\n",
        "# Function to convert a document to a sequence of words,\n",
        "# optionally removing stop words. Returns a list of words.\n",
        "#\n",
        "# 1. Remove HTML\n",
        "    review_text = BeautifulSoup(review).get_text()\n",
        "#\n",
        "# 2. Remove non-letters\n",
        "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
        "#\n",
        "# 3. Convert words to lower case and split them\n",
        "    words = review_text.lower().split()\n",
        "#\n",
        "# 4. Optionally remove stop words (false by default)\n",
        "    if remove_stopwords:\n",
        "        stops = set(stopwords.words(\"english\"))\n",
        "        words = [w for w in words if not w in stops]\n",
        "#\n",
        "# 5. Return a list of words\n",
        "    return(words)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4Bi8oMQNTg0",
        "colab_type": "code",
        "outputId": "a0fc05e8-8360-4f32-ece0-5519c4fbb559",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# Download the punkt tokenizer for sentence splitting\n",
        "import nltk.data\n",
        "nltk.download('punkt')\n",
        "# Load the punkt tokenizer\n",
        "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "# Define a function to split a review into parsed sentences\n",
        "def review_to_sentences( review, tokenizer, remove_stopwords=False ):\n",
        "# Function to split a review into parsed sentences. Returns a\n",
        "# list of sentences, where each sentence is a list of words\n",
        "#\n",
        "# 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
        "    raw_sentences = tokenizer.tokenize(review.strip())\n",
        "#\n",
        "# 2. Loop over each sentence\n",
        "    sentences = []\n",
        "    for raw_sentence in raw_sentences:\n",
        "# If a sentence is empty, skip it\n",
        "        if len(raw_sentence) > 0:\n",
        "# Otherwise, call review_to_wordlist to get a list of words\n",
        "            sentences.append( review_to_wordlist( raw_sentence, remove_stopwords ))\n",
        "#\n",
        "# Return the list of sentences (each sentence is a list of words,\n",
        "# so this returns a list of lists\n",
        "    return sentences\n"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLymiuSicQGL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "path = '/content/gdrive/My Drive/data/labeledTrainData.tsv'\n",
        "\n",
        "data1 = pd.read_csv(path, header=0, delimiter=\"\\t\", quoting=3)\n",
        "data = data1['review']\n",
        "target = data1['sentiment']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kEv3xQleARV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybSrRiltdYO2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn import datasets, linear_model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# create training and testing vars\n",
        "train, test, ytrain, ytest = train_test_split(data, target, test_size=0.2)\n",
        "#print X_train.shape, y_train.shape\n",
        "#print X_test.shape, y_test.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8IIPXlQXy4Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xtrain = train.values.reshape(-1,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tITpCjRKhGYJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "313fbf31-1571-4308-e769-853da5b3d580"
      },
      "source": [
        "xtrain"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([['\"What a load of Leftist Hollywood bilge. This movie glorifies mutiny as brave and noble if it be for pacifist principles. The fairytale ends with the pacifist character, played by Danzel Washington, actually getting promoted for his treason. What is it with these Hollywood tools? Is this still payback for McCarthyism?<br /><br />If I sound cynical it\\'s because I am fed up with movies hawking a political agenda. The military brass in this movie are portrayed as, what else? Gung-ho war mongers. Sound familiar? Ever see a movie where the CIA or any government agency is not evil? Think about it. Yet again, Crimson Tide stresses the point. The Hackman character, U-boat captain Ramsey, comes across like a raving lunatic, until the very end when, of course he comes to his senses, does a complete 360, renounces his blood lust, suggests a promotion for the treasonous Ron Hunter, and repents by retiring from the service. A guy mutinies, takes command of your boat, puts the U.S at grave risk of receiving a nuclear first-strike, and you promote him???? What hogwash!\"'],\n",
              "       ['\"Jesus Christ, I can\\'t believe I\\'ve wasted my time watching this movie. I only watched because I have such a crush on Jordan Ladd. But watching this film almost put me off her. This is absolutely awful! I could have been watching Survivor Series 93 over this.<br /><br />The lead guy in this was so bland and generic. I would love it if the great Mistuharu Misawa Tiger Drove \\'91\\'d his ass through a glass window. I was enraging every time he was saying \\\\\"lake\\\\\" and \\\\\"cabin\\\\\". I\\'d kick his ass.<br /><br />Jordan Ladd, on the other hand, was absolutely wonderful. A true angel. But she couldn\\'t even save this utter joke of a film. Sadly, she couldn\\'t even act like she was off her nut when she took that truth drug. It looked hilarious.<br /><br />I also loved the bit where Jordan accidentally spilled yogurt on her. It reminded me of a time where...nevermind.<br /><br />Anayways, do watch this film because of it\\'s awfulness.\"'],\n",
              "       ['\"*** REVIEW MAY CONTAIN SOME SPOILERS *** I\\'ll make this review short and sweet. I bought this movie from Best Buy because it sounded interested and had some top actors in it like Kevin Spacey and Morgan Freeman. How bad could it be, right? Well, it\\'s pretty bad. Justin Timberlake plays Pollack, a wannabe journalist who stumbles across a case that may lead to corrupt cops at Edison\\'s Police Force. LL Cool J is Deed, a cop within the force on a special force team called F.R.A.T. (First Response Assault Tactics). He\\'s teamed with an \\\\\"on-the-edge\\\\\" bad cop named Lazerov (Dylan McDermott). In the opening scene we see Lazerov & Deed taking on some bank robbers, but at night they are busting a couple of guys doing drugs. I don\\'t want to give to much away, but things turn bad for the guys doing the drugs. Pollack, who works for Ashford (Morgan Freeman) goes to a trial involving Deeds & Lazerov. He suspect foul play and with the help of Ashford, does some investigate that turns ugly. Wallace (Kevin Spacey) who is all within the F.R.A.T. team joins with Ashford to try to bring the corrupt cops to justice.<br /><br />You can tell from the beginning that Freeman and Spacey\\'s performance are pretty lackluster. The only person that give a all out performance is Dylan McDermott. He is a complete nut case in this movie and made a believer out of me. LL Cool J is terrible in this film. He says every line the same way and shows pretty much the same emotion. He was much better in movies like Deep Blue Sea & Any Given Sunday. The film starts off with some nice action but then drags it feet through the rest of the film. The ending is far from satisfying.<br /><br />Don\\'t waste your time with this film. I\\'m putting it on Ebay this weekend.\"'],\n",
              "       ...,\n",
              "       ['\"Coinciding with the start of the baby boom, the years after World War II saw an unprecedented exodus of Americans moving out of their city apartments into the suburbs where they can fulfill their dreams of owning their own homes. Directed by H.C. Potter and co-written by Norman Panama and Melvin Frank (\\\\\"White Christmas\\\\\"), this lightweight but surprisingly observant 1948 screwball comedy captures the feeling of that period very well. Of course, it helps to have a trio of expert farceurs \\x96 Cary Grant, Myrna Loy and an especially acerbic Melvyn Douglas \\x96 head the proceedings with their natural likability at odds with the escalating frustrations of home ownership. Even though the film is sixty years old now, there is a timeless quality to the Blandings\\' dream and the barriers they face in achieving it. Obviously, Hollywood thinks so since it\\'s been remade at least twice - first as a very physical Tom Hanks comedy, 1986\\'s \\\\\"The Money Pit\\\\\", and again last year with Ice Cube\\'s \\\\\"Are We Done Yet?\\\\\". One look at HGTV\\'s programming schedule will show you how the situations explored here still resonate today.<br /><br />The plot begins with ad man Jim Blandings, his wife Muriel and their two daughters cramped into a two bedroom-one bath Manhattan apartment. Rather than pursue Muriel\\'s idea to renovate the apartment for $7,000, Jim sees a photo of a Connecticut house in a magazine and realizes this is where they need to move. With the help of an opportunistic real estate agent and against the advice of their attorney and family friend Bill Cole, the Blandings decide to buy a ramshackle house badly in need of repair. However, the foundation sags so badly that the house needs to be torn down in favor of a new one. This sparks the Blandings to push the architect to design a house so excessive that the second floor is twice as big as the first. Costs rise with each new complication, tempers flare, and even a romantic triangle is imagined among, Jim, Muriel and Bill. Priorities finally sort themselves out but not before some funny slapstick scenes and clever dialogue that tweaks the not-so-blissful ignorance of the new homeowners.<br /><br />With his double takes and flawless line delivery, Grant is infallible in this type of farce, and Jim Blandings epitomizes his more domesticated mid-career characters. In a role originally meant for Irene Dunne, Myrna Loy shows why she was Hollywood\\'s perfect wife. She doesn\\'t get many of the funnier lines, but she combines her special blend of flightiness and sauciness to make Muriel an appealing character on her own. Watch her deftly maneuver the overly agreeable house painter with her absurdly idiosyncratic color palette. As avuncular, pipe-smoking Bill (\\\\\"Cole\\x85Bill Cole\\\\\"), Melvyn Douglas shows his natural, easy-going élan as Grant\\'s foil. Smaller roles are filled expertly with particularly memorable turns by Harry Shannon as the laconic well-digger Mr. Tesander, Lurene Tuttle as Jim\\'s officious assistant Mary, and Louise Beavers as the Blandings\\' lovable maid Gussie. The 2004 DVD provides some intriguing vintage material including two radio versions of the movie - the first a 1949 version that did end up pairing Grant and Dunne and then a second 1950 version coupling Grant with his then-wife, actress Betsy Drake. A most appropriate 1949 cartoon, \\\\\"The House of Tomorrow\\\\\", is also included giving us a comical tour of a futuristic dream house. The original theatrical trailers for ten of Grant\\'s film classics complete the extras.\"'],\n",
              "       ['\"I never bothered to see this movie in theaters although I remember hearing the name over and over. I finally watched it this week and what a delight. For some reason, I was expecting it to not be very good so I was completely surprised when I sat down and stuck with it and then found myself completely pulled in. I read a lot of the other user comments and it impressed me how much people talk about her fighting in the ring, but what was wonderful about Diana is that she\\'s a true fighter in life. All she needs to do is find her place where she be who she is and the ring helps her to get there. A very intelligent story and I\\'m amazed that this is the first time up for Michelle Rodriguez - what an excellent job she did. Adrian and her coach were also quite good.<br /><br />This film is a little rough around the edges, but it doesn\\'t matter in the slightest. The story, the will, and the performances completely outweigh any flaws (that usually come with indie filmmaking anyway). A compelling portrayal of a girl finding herself and triumphing over her circumstances and a K.O. for Michelle Rodriguez!\"'],\n",
              "       ['\"First off, anyone who thinks this sequel to William Friedkin\\'s \\\\\"The French Connection\\\\\", is superior is most definitely completely insane or moronic or both. The problem with reviewing this film is that, a.) it\\'s a sequel to a brilliant movie, which always makes watching it objectively difficult, and b.) it\\'s directed by John Frankenhimer, one of the best American directors ever, so I wanted to like it. William Friendkin was the perfect person to direct a film about drug traffic in decaying new York city, because of his documentary-like approach to the action and story, Frankenhimer on the other hand is one of the most stylish directors ever, i.e. \\\\\"The Manchurian Candidate\\\\\" and \\\\\"Seconds\\\\\", and with his \\\\\"French Connection 2\\\\\" it feels like someone trying to be gritty and not having the true understanding to pull it off. That fact that Frankenhimer was chosen to direct the sequel by Gene Hackman himself really tells a lot about Hackman\\'s understanding about the original film too. It\\'s well known Hackman hated Friedkin on the set and vowed to never work with him again, it\\'s also known he envisioned the character to be more one dimensional, loosing weight and trying to play him like a straight character. it shows you Hackman, despite being a great actor, had no idea who to make the movie and the story great. The plot point of Doyle becoming an addict is interesting, but doesn\\'t warrant the rest of the film. An unfortunate low point in Frankenhimer\\'s filmography.\"']],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6DIOdTyO3IW",
        "colab_type": "code",
        "outputId": "821d3e12-cc30-47d8-f24d-c87d57986db7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "'''\n",
        "from google.colab import drive\n",
        "import glob\n",
        "import os\n",
        "'''\n",
        "'''\n",
        "drive.mount('/content/gdrive/')\n",
        "!ls \"/content/gdrive/My Drive\"\n",
        "download_path = os.path.expanduser(\"/content/gdrive/My Drive\")\n",
        "try:\n",
        "  os.makedirs(download_path)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "  \n",
        "'''\n",
        "'''\n",
        "drive.mount('/content/gdrive/')\n",
        "\n",
        "\n",
        "def read_files1(path):\n",
        "    \n",
        "    #drive.mount('/content/gdrive/')\n",
        "\n",
        "    !mkdir drive\n",
        "    !google-drive-ocamlfuse drive\n",
        "    !ls \"gdrive/My Drive/aclImdb\"\n",
        "\n",
        "    print(path)\n",
        "    !ls path\n",
        "    #path = glob.glob('/gdrive/aclImdb/train/pos/')\n",
        "    print(path)\n",
        "    documents = list()\n",
        "    print(\"11111\")\n",
        "    i = 0\n",
        "    for filename in os.listdir(path):\n",
        "      with open('%s/%s' % (path, filename),encoding='utf-8') as f:\n",
        "        print(\"2222\")\n",
        "        i = i+1\n",
        "        print(i)\n",
        "        doc = f.read()\n",
        "        documents.append(doc)\n",
        "        if i == 4:\n",
        "          break\n",
        "        \n",
        "    if os.path.isfile(path):\n",
        "        with open(path, encoding='iso-8859-1') as f:\n",
        "            doc = f.readlines()\n",
        "            for line in doc:\n",
        "                documents.append(line)\n",
        "    \n",
        "    return documents\n",
        "'''"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ndrive.mount(\\'/content/gdrive/\\')\\n\\n\\ndef read_files1(path):\\n    \\n    #drive.mount(\\'/content/gdrive/\\')\\n\\n    !mkdir drive\\n    !google-drive-ocamlfuse drive\\n    !ls \"gdrive/My Drive/aclImdb\"\\n\\n    print(path)\\n    !ls path\\n    #path = glob.glob(\\'/gdrive/aclImdb/train/pos/\\')\\n    print(path)\\n    documents = list()\\n    print(\"11111\")\\n    i = 0\\n    for filename in os.listdir(path):\\n      with open(\\'%s/%s\\' % (path, filename),encoding=\\'utf-8\\') as f:\\n        print(\"2222\")\\n        i = i+1\\n        print(i)\\n        doc = f.read()\\n        documents.append(doc)\\n        if i == 4:\\n          break\\n        \\n    if os.path.isfile(path):\\n        with open(path, encoding=\\'iso-8859-1\\') as f:\\n            doc = f.readlines()\\n            for line in doc:\\n                documents.append(line)\\n    \\n    return documents\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43DtcD11O3Gi",
        "colab_type": "code",
        "outputId": "2cf7dad4-d28a-4b89-8342-cd0911dfbb14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "'''\n",
        "pos_train = read_files1('/content/gdrive/My Drive/aclImdb/train/pos')\n",
        "neg_train = read_files1('/content/gdrive/My Drive/aclImdb/train/neg')\n",
        "pos_test = read_files1('/content/gdrive/My Drive/aclImdb/test/pos')\n",
        "neg_test = read_files1('/content/gdrive/My Drive/aclImdb/test/neg')\n",
        "\n",
        "\n",
        "train1 = pos_train + neg_train\n",
        "test1 = pos_test + neg_test\n",
        "\n",
        "#docs = negative_docs + positive_docs\n",
        "l_train = [1 for _ in range(len(pos_train))] + [0 for _ in range(len(neg_train))]\n",
        "l_test = [1 for _ in range(len(pos_train))] + [0 for _ in range(len(neg_test))]\n",
        "train = np.column_stack((train1,l_train))\n",
        "test = np.column_stack((test1,l_test))\n",
        "'''"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\npos_train = read_files1('/content/gdrive/My Drive/aclImdb/train/pos')\\nneg_train = read_files1('/content/gdrive/My Drive/aclImdb/train/neg')\\npos_test = read_files1('/content/gdrive/My Drive/aclImdb/test/pos')\\nneg_test = read_files1('/content/gdrive/My Drive/aclImdb/test/neg')\\n\\n\\ntrain1 = pos_train + neg_train\\ntest1 = pos_test + neg_test\\n\\n#docs = negative_docs + positive_docs\\nl_train = [1 for _ in range(len(pos_train))] + [0 for _ in range(len(neg_train))]\\nl_test = [1 for _ in range(len(pos_train))] + [0 for _ in range(len(neg_test))]\\ntrain = np.column_stack((train1,l_train))\\ntest = np.column_stack((test1,l_test))\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJLEw5dyZt9p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPA-qWaOWT2p",
        "colab_type": "code",
        "outputId": "ac99b4b8-e66d-498b-a77a-3b91a8a9aff8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "sentences = []  # Initialize an empty list of sentences\n",
        "\n",
        "print (\"Parsing sentences from training set\")\n",
        "for review in data1[\"review\"]:\n",
        "    sentences += review_to_sentences(review, tokenizer)\n"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parsing sentences from training set\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:273: UserWarning: \"b'.'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
            "  ' Beautiful Soup.' % markup)\n",
            "/usr/local/lib/python3.6/dist-packages/bs4/__init__.py:336: UserWarning: \"http://www.happierabroad.com\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
            "  ' that document to Beautiful Soup.' % decoded_markup\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAJzUW4ki-C_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 572
        },
        "outputId": "5b100677-e77d-4b2f-bea7-71ac33f23865"
      },
      "source": [
        "sentences[0]"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['with',\n",
              " 'all',\n",
              " 'this',\n",
              " 'stuff',\n",
              " 'going',\n",
              " 'down',\n",
              " 'at',\n",
              " 'the',\n",
              " 'moment',\n",
              " 'with',\n",
              " 'mj',\n",
              " 'i',\n",
              " 've',\n",
              " 'started',\n",
              " 'listening',\n",
              " 'to',\n",
              " 'his',\n",
              " 'music',\n",
              " 'watching',\n",
              " 'the',\n",
              " 'odd',\n",
              " 'documentary',\n",
              " 'here',\n",
              " 'and',\n",
              " 'there',\n",
              " 'watched',\n",
              " 'the',\n",
              " 'wiz',\n",
              " 'and',\n",
              " 'watched',\n",
              " 'moonwalker',\n",
              " 'again']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49_bGNsNO3Ek",
        "colab_type": "code",
        "outputId": "f59d4a66-c963-49e9-9cf2-475465c685bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571
        }
      },
      "source": [
        "sentences = [] # Initialize an empty list of sentences\n",
        "num_reviews = 20000\n",
        "print (\"Parsing sentences from training set\")\n",
        "for review in range( 1, num_reviews ):\n",
        "  if(review !=6):\n",
        "    sentences += review_to_sentences(train[review], tokenizer)\n",
        "'''\n",
        "print (\"Parsing sentences from unlabeled set\")\n",
        "for review in unlabeled_train[\"review\"]:\n",
        "    sentences += review_to_sentences(review, tokenizer)\n",
        "'''"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parsing sentences from training set\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-79-8b5b5d1667c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mreview\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_reviews\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview\u001b[0m \u001b[0;34m!=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreview_to_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m '''\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Parsing sentences from unlabeled set\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    866\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    869\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, series, key)\u001b[0m\n\u001b[1;32m   4373\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4374\u001b[0m             return self._engine.get_value(s, k,\n\u001b[0;32m-> 4375\u001b[0;31m                                           tz=getattr(series.dtype, 'tz', None))\n\u001b[0m\u001b[1;32m   4376\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4377\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mholds_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_boolean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 3"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnFRlwSpO3CY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',    level=logging.INFO)\n",
        "\n",
        "def myhashfxn(obj):\n",
        "    return hash(obj) % (2 ** 32)\n",
        "\n",
        "# Set values for various parameters\n",
        "num_features = 400    # Word vector dimensionality                      \n",
        "min_word_count = 60   # Minimum word count                        \n",
        "num_workers = 4       # Number of threads to run in parallel\n",
        "context = 10          # Context window size                                                                                    \n",
        "downsampling = 1e-3   # Downsample setting for frequent words\n",
        "\n",
        "\n",
        "#**********************************************************\n",
        "\n",
        "\n",
        "\n",
        "# Initialize and train the model (this will take some time)\n",
        "from gensim.models import word2vec\n",
        "#model = word2vec.Word2Vec(hashfxn=myhashfxn)\n",
        "\n",
        "print (\"Training model...\")\n",
        "model = word2vec.Word2Vec(sentences, workers=num_workers,             size=num_features, min_count = min_word_count,             window = context, sample = downsampling,hashfxn=myhashfxn)\n",
        "\n",
        "# If you don't plan to train the model any further, calling \n",
        "# init_sims will make the model much more memory-efficient.\n",
        "model.init_sims(replace=True)\n",
        "\n",
        "# It can be helpful to create a meaningful model name and \n",
        "# save the model for later use. You can load it later using Word2Vec.load()\n",
        "model_name = \"300features_40minwords_10context\"\n",
        "model.save(model_name)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRHjnKP3O2-1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.doesnt_match(\"man woman child kitchen\".split())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HrctvxdO26f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "model.most_similar(\"man\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jujvNDbPKve",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Vector Averaging\n",
        "import numpy as np # Make sure that numpy is imported\n",
        "def makeFeatureVec(words, model, num_features):\n",
        "# Function to average all of the word vectors in a given\n",
        "# paragraph\n",
        "#\n",
        "# Pre-initialize an empty numpy array (for speed)\n",
        "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
        "#\n",
        "    nwords = 0.\n",
        "#\n",
        "# Index2word is a list that contains the names of the words in\n",
        "# the model's vocabulary. Convert it to a set, for speed\n",
        "    index2word_set = set(model.wv.index2word)\n",
        "#\n",
        "# Loop over each word in the review and, if it is in the model's\n",
        "# vocaublary, add its feature vector to the total\n",
        "    for word in words:\n",
        "        if word in index2word_set:\n",
        "            nwords = nwords + 1.\n",
        "            featureVec = np.add(featureVec,model[word])\n",
        "#\n",
        "# Divide the result by the number of words to get the average\n",
        "    featureVec = np.divide(featureVec,nwords)\n",
        "    return featureVec\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c058MlCUPK3n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getAvgFeatureVecs(reviews, model, num_features):\n",
        "# Given a set of reviews (each one a list of words), calculate\n",
        "# the average feature vector for each one and return a 2D numpy array\n",
        "#\n",
        "# Initialize a counter\n",
        "    counter = 0\n",
        "#\n",
        "# Preallocate a 2D numpy array, for speed\n",
        "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
        "# Loop through the reviews\n",
        "    for review in reviews:\n",
        "        if counter%1000 == 0:\n",
        "            print (\"Review %d of %d\" % (counter, len(reviews)))\n",
        "        reviewFeatureVecs[counter] = makeFeatureVec(review, model,num_features)\n",
        "#\n",
        "# Increment the counter\n",
        "        counter = counter + 1\n",
        "    return reviewFeatureVecs\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjB4MMtqPLCN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clean_train_reviews = []\n",
        "for review in train1:\n",
        "    clean_train_reviews.append( review_to_wordlist( review,         remove_stopwords=True ))\n",
        "    \n",
        "trainDataVecs = getAvgFeatureVecs( clean_train_reviews, model, num_features )\n",
        "print (\"Creating average feature vecs for test reviews\")\n",
        "clean_test_reviews = []\n",
        "for review in test1:\n",
        "    clean_test_reviews.append( review_to_wordlist( review,         remove_stopwords=True ))\n",
        "\n",
        "\n",
        "# In[52]:"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajbzoLmrPK_I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DHiVhllPK0z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDoM4k_bPKyo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}