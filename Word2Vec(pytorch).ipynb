{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled3.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7yYE7X7_mgS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os, pickle, re, sklearn, string\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "85f4443c-d42b-4b4f-a9db-3226cefecfe2",
        "id": "177cZi2OjAWT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')# Download text data sets, including stop words\n",
        "from nltk.corpus import stopwords # Import the stop word list\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "\n",
        "# Download the punkt tokenizer for sentence splitting\n",
        "import nltk.data\n",
        "nltk.download('punkt')   \n",
        "\n",
        "# Load the punkt tokenizer\n",
        "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "\n",
        "class Creat_vocabulary():\n",
        "  \n",
        "  def __init__(self):\n",
        "    self.sentences = []\n",
        "    self.word2idx = {}\n",
        "    self.idx2word = {}\n",
        "    self.idx_pairs = []\n",
        "    self.all_voc = []\n",
        "    self.negative_pairs = []\n",
        "    \n",
        "    \n",
        "  def get_sentence(self):\n",
        "    \n",
        "      return self.sentences\n",
        "    \n",
        "    \n",
        "  def get_word2idx(self):\n",
        "    \n",
        "      return self.word2idx\n",
        "  \n",
        "  \n",
        "  def get_idx2word(self):\n",
        "    \n",
        "      return self.idx2word\n",
        "  \n",
        "  \n",
        "  def get_idx_pairs(self):\n",
        "    \n",
        "      return self.idx_pairs\n",
        "    \n",
        "  def expand(self,ListofList):\n",
        "    \n",
        "    for ll in ListofList:\n",
        "      for l in ll:\n",
        "        self.all_voc.append(l)\n",
        "    return self.all_voc   \n",
        "  \n",
        "\n",
        "  # Define a function to split a review into parsed sentences\n",
        "  def review_to_sentences( self, review, tokenizer, remove_stopwords=False ):\n",
        "      # Function to split a review into parsed sentences. Returns a \n",
        "      # list of sentences, where each sentence is a list of words\n",
        "    \n",
        "      # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
        "      raw_sentences = tokenizer.tokenize(review.strip())\n",
        "    \n",
        "    \n",
        "      # 2. Loop over each sentence\n",
        "      sentences = []\n",
        "      for raw_sentence in raw_sentences:\n",
        "      \n",
        "          # If a sentence is empty, skip it\n",
        "          if len(raw_sentence) > 0:\n",
        "          \n",
        "              # Otherwise, call review_to_wordlist to get a list of words\n",
        "              sentences.append( self.review_to_wordlist( raw_sentence,remove_stopwords ))\n",
        "    \n",
        "      # Return List os List of Tokens in a sentence\n",
        "      return sentences\n",
        "\n",
        "  \n",
        "  def review_to_wordlist(self, review, remove_stopwords=False ):\n",
        "  \n",
        "    # Function to convert a document to a sequence of words,\n",
        "    # optionally removing stop words. Returns a list of words.\n",
        "\n",
        "    # 1. Remove HTML\n",
        "    review_text = BeautifulSoup(review).get_text()\n",
        "\n",
        "    # 2. Remove non-letters\n",
        "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
        "\n",
        "    # 3. Convert words to lower case and split them\n",
        "    words = review_text.lower().split()\n",
        "\n",
        "    # 4. Optionally remove stop words (false by default)\n",
        "    if remove_stopwords:\n",
        "        stops = set(stopwords.words(\"english\"))\n",
        "        words = [w for w in words if not w in stops]\n",
        "\n",
        "    # 5. Return a list of words\n",
        "    return(words)\n",
        "\n",
        "\n",
        "  def reviews_to_Words(self,data, tokenizer):\n",
        "  \n",
        "    sentences = []  # Initialize an empty list of sentences\n",
        "\n",
        "    print (\"Parsing sentences from training set\")\n",
        "  \n",
        "    for review in data[\"review\"]:\n",
        "      self.sentences += self.review_to_sentences(review, tokenizer)\n",
        "  \n",
        "    return self.sentences\n",
        "  \n",
        " \n",
        "  \n",
        "  def dictionary(self, tokenizer ):\n",
        "    \n",
        "    vocabulary = []\n",
        "    for sentence in self.sentences:\n",
        "      for token in sentence:\n",
        "          if token not in vocabulary:\n",
        "              vocabulary.append(token)\n",
        "\n",
        "    self.word2idx = {w: idx for (idx, w) in enumerate(vocabulary)}\n",
        "    self.idx2word = {idx: w for (idx, w) in enumerate(vocabulary)}\n",
        "\n",
        "    vocabulary_size = len(vocabulary)\n",
        "    \n",
        "    return vocabulary_size, self.word2idx, self.idx2word\n",
        "  \n",
        "  def skip_contex_tuple(self,context_size):\n",
        "    \n",
        "    \n",
        "    # changing context_size to window size\n",
        "    if(context_size % 2 != 0):\n",
        "      window_size = int(context_size + 1)//2\n",
        "    else:\n",
        "      window_size = int(context_size)//2\n",
        "  \n",
        "    #find context words around each word in a sentence and then coolect it in a list using vocab index in our dic\n",
        "    for sentence in self.sentences:\n",
        "      \n",
        "      \n",
        "      indices = [self.word2idx[word] for word in sentence]\n",
        "      \n",
        "      # for each word, threated as center word\n",
        "      for center_word_pos in range(len(indices)):\n",
        "        tup =[]\n",
        "        \n",
        "        for w in range(-window_size, window_size):\n",
        "          cntx_pos = center_word_pos + w \n",
        "          \n",
        "\n",
        "          if cntx_pos <=0 or cntx_pos>=len(indices) or cntx_pos == center_word_pos:\n",
        "\n",
        "            continue\n",
        "            \n",
        "          tup.append(self.word2idx[sentence[cntx_pos]])\n",
        "            \n",
        "        # for each window position\n",
        "        #tup = [sentence[center_word_pos + w ] for w in range(-window_size, window_size) if center_word_pos + w > 0 and center_word_pos + w <= len(indices) and center_word_pos  != center_word_pos + w ]\n",
        "\n",
        "        self.idx_pairs.append((indices[center_word_pos], tup,1))\n",
        "\n",
        "    self.idx_pairs = np.array(self.idx_pairs) # it will be useful to have this as numpy array\n",
        "    \n",
        "    return self.idx_pairs\n",
        "  \n",
        "  \n",
        "  def get_negatives(self,idx_pairs, sampling_weights, K):\n",
        "    \n",
        "    all_negatives = []\n",
        "    \n",
        "    generator = RandomGenerator(sampling_weights)\n",
        "    \n",
        "    for pair in idx_pairs:\n",
        "        negatives = []\n",
        "        contexts = pair[1]\n",
        "        center = pair[0]\n",
        "        while len(negatives) <=  K:\n",
        "          \n",
        "            neg = generator.draw()\n",
        "            \n",
        "            # Noise words cannot be context words\n",
        "            if neg not in contexts:\n",
        "              \n",
        "                negatives.append(neg)\n",
        "                \n",
        "        self.negative_pairs.append((center, negatives,0))\n",
        "     \n",
        "    self.negative_pairs = np.array(self.negative_pairs)\n",
        "        \n",
        "    return self.negative_pairs"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nD6Xw5APKD8k",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njWHoCLZV8DO",
        "colab_type": "code",
        "outputId": "7cd231e4-7fba-48d6-d18e-afde1a175533",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "\n",
        "import random\n",
        "\n",
        "class RandomGenerator(object):\n",
        "    \"\"\"Draw a random int in [0, n] according to n sampling weights\"\"\"\n",
        "    def __init__(self, sampling_weights):\n",
        "        self.population = list(range(len(sampling_weights)))\n",
        "        self.sampling_weights = sampling_weights\n",
        "        self.candidates = []\n",
        "        self.i = 0\n",
        "\n",
        "    def draw(self):\n",
        "        if self.i == len(self.candidates):\n",
        "            self.candidates = random.choices(\n",
        "                self.population, self.sampling_weights, k=10000)\n",
        "            self.i = 0\n",
        "        self.i += 1\n",
        "        return self.candidates[self.i-1]\n",
        "\n",
        "      \n",
        "      \n",
        "      \n",
        "\n",
        "generator = RandomGenerator([2,3,4])\n",
        "[generator.draw() for _ in range(10)]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 2, 2, 1, 1, 1, 0, 2, 1, 2]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLymiuSicQGL",
        "colab_type": "code",
        "outputId": "2455a662-fe56-472f-b0a6-115569cdc960",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "import glob\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "#in order to use the dataset from MyDrive, need to mount it in google colab\n",
        "drive.mount('/content/gdrive/')\n",
        "path = '/content/gdrive/My Drive/data/labeledTrainData.tsv'\n",
        "\n",
        "data2 = pd.read_csv(path, header=0, delimiter=\"\\t\", quoting=3)\n",
        "data = data2['review']\n",
        "target = data2['sentiment']\n",
        "\n",
        "data1 = data2.iloc[1:200]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybSrRiltdYO2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn import datasets, linear_model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# create training and testing vars\n",
        "train, test, ytrain, ytest = train_test_split(data, target, test_size=0.2)\n",
        "#print X_train.shape, y_train.shape\n",
        "#print X_test.shape, y_test.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8IIPXlQXy4Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xtrain = train.values.reshape(-1,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPA-qWaOWT2p",
        "colab_type": "code",
        "outputId": "c8d76f00-08d9-4899-c9c6-e2c3f5d3e4b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "sentences = []  # Initialize an empty list of sentences\n",
        "vocab =  Creat_vocabulary()\n",
        "sentences =vocab.reviews_to_Words(data1 , tokenizer)\n",
        "\n",
        "vocabulary_size, word2idx, idx2word = vocab.dictionary( tokenizer )"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parsing sentences from training set\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umPhbsODV_iH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import collections\n",
        "\n",
        "listofV= vocab.get_sentence()\n",
        "\n",
        "counter = collections.Counter(vocab.expand(listofV))\n",
        "\n",
        "\n",
        "counters = list(counter.values())\n",
        "\n",
        "\n",
        "sampling_weights = [counters[i]**0.75 for i in range(len(counters))]\n",
        "\n",
        "vocab_size = len(counter)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gceLeFbPYW46",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "idx_pairs = vocab.skip_contex_tuple(3)\n",
        "\n",
        "all_negatives = vocab.get_negatives(idx_pairs, sampling_weights, 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pV2SHWo6C7s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CONTEXT_SIZE = len(all_negatives) + len(idx_pairs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8g53tvD3p13",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "e709cc05-e345-4c3c-e376-20c95fd6ae6a"
      },
      "source": [
        "all_negatives"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, list([281, 1146, 573]), 0],\n",
              "       [1, list([92, 2243, 19]), 0],\n",
              "       [2, list([4540, 435, 4925]), 0],\n",
              "       ...,\n",
              "       [3080, list([2280, 2456, 1770]), 0],\n",
              "       [0, list([3580, 4253, 1770]), 0],\n",
              "       [4010, list([2535, 2672, 3518]), 0]], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86ta_1Nc59jG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "\n",
        "\n",
        "\n",
        "class Skipgram(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
        "      \n",
        "        super(Skipgram, self).__init__()\n",
        "        \n",
        "        \n",
        "        self.embed_v = nn.Embedding(vocab_size, embedding_dim)\n",
        "        \n",
        "        self.embed_u = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        \n",
        "        #self.linear1 = nn.Linear(embedding_dim, 128)\n",
        "        #self.linear2 = nn.Linear(128, context_size * vocab_size)\n",
        "        #self.parameters['context_size'] = context_size\n",
        "\n",
        " \n",
        "\n",
        "    def forward(self,center, contexts):\n",
        "      \n",
        "        v = self.embed_v(center)\n",
        "        u = self.embed_u(contexts)\n",
        "        pred = torch.mm(v, u.view(200,1))\n",
        "        #log_probs = F.softmax(pred, dim=1).view(1)\n",
        "        log_probs = pred\n",
        "        #log_probs = F.log_softmax(pred, dim=1)\n",
        "        #log_probs = log_probs[None,:]\n",
        "        #print(log_probs.size())\n",
        "        return log_probs\n",
        "\n",
        "losses = []\n",
        "EMBEDDING_DIM = 200\n",
        "CONTEXT_SIZE = len(all_negatives) + len(idx_pairs)\n",
        "loss_function =nn.BCEWithLogitsLoss()\n",
        "model = Skipgram(vocab_size, EMBEDDING_DIM, CONTEXT_SIZE)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUJVQyAjLH1R",
        "colab_type": "code",
        "outputId": "d233ed99-354f-4b38-c3f4-4415cf9df0fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "\n",
        "\n",
        "'''\n",
        "context_size = 3\n",
        "embedding_dim = 126\n",
        "\n",
        "model = nn.Sequential(\n",
        "    \n",
        "    \n",
        "                      nn.Embedding(vocabulary_size, embedding_dim),\n",
        "                      nn.Linear(embedding_dim,256),\n",
        "                      nn.Linear(256,context_size * vocabulary_size),\n",
        "                      nn.LogSoftmax(dim=1))\n",
        "\n",
        "\n",
        "\n",
        "objective = nn.NLLLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.003)\n",
        "'''\n",
        "\n",
        "from torch.autograd import Variable\n",
        "\n",
        "losses= []\n",
        "epoch = 4\n",
        "tar = [0,1]\n",
        "target = torch.tensor(tar, dtype=torch.long)\n",
        "#target = target.view(1,-1)\n",
        "for e in range(epoch):\n",
        "  \n",
        "  running_loss = 0\n",
        "\n",
        "  for i in range(0,len(idx_pairs)):\n",
        "    \n",
        "    output =[]\n",
        "    label = []\n",
        "    for context in idx_pairs[i][1]:\n",
        "      \n",
        "        context = torch.tensor([context])\n",
        "        center = torch.tensor([idx_pairs[i][0]])\n",
        "        label1 = torch.tensor([idx_pairs[i][2]])\n",
        "        aa=  model.forward(center, context)\n",
        "        #print(aa)\n",
        "        output.append(aa)\n",
        "        label.append(label1)\n",
        "        \n",
        "        \n",
        "    for negative in all_negatives[i][1]:\n",
        "                      \n",
        "        context = torch.tensor([negative], dtype=torch.long)\n",
        "        center = torch.tensor([all_negatives[i][0]], dtype=torch.long)\n",
        "        #label0 = torch.tensor([all_negatives[i][2]])\n",
        "        label0 = torch.tensor([0])\n",
        "        output.append(model.forward(center, context))\n",
        "        label.append(label0)\n",
        "\n",
        "        \n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    output = Variable(torch.tensor([output], dtype=torch.float32),requires_grad=True)\n",
        "    labelz = torch.tensor([label], dtype=torch.float32)\n",
        "\n",
        "\n",
        "    loss = loss_function(output,labelz)    \n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "        \n",
        "    running_loss += loss.item()\n",
        "    losses.append(loss.item())\n",
        "        \n",
        "      \n",
        "  else:\n",
        "        print(f\"Training loss: {running_loss}\")"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-62-a804e84bd54e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;31m#label0 = torch.tensor([all_negatives[i][2]])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mlabel0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcenter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-61-e29a85562ec9>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, center, contexts)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_v\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcenter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_u\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0;31m#log_probs = F.softmax(pred, dim=1).view(1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mlog_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mLrQrpNNPAv",
        "colab_type": "code",
        "outputId": "db4db20e-fa90-4fb0-9a7c-35f9f3c14c17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17368
        }
      },
      "source": [
        "losses"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[10.372519493103027,\n",
              " 1.3936128616333008,\n",
              " 10.163941383361816,\n",
              " 3.060337543487549,\n",
              " 9.068787574768066,\n",
              " 2.5191872119903564,\n",
              " 5.538202285766602,\n",
              " 7.9376044273376465,\n",
              " 4.872311115264893,\n",
              " 9.865071296691895,\n",
              " 5.591497421264648,\n",
              " 8.310934066772461,\n",
              " 3.3092963695526123,\n",
              " 8.338900566101074,\n",
              " 7.13659143447876,\n",
              " 7.206770896911621,\n",
              " 5.276698589324951,\n",
              " 3.4356536865234375,\n",
              " 6.133111953735352,\n",
              " 4.971600532531738,\n",
              " 6.034645080566406,\n",
              " 6.406653881072998,\n",
              " 11.759329795837402,\n",
              " 9.66158676147461,\n",
              " 4.148512363433838,\n",
              " 7.502174377441406,\n",
              " 9.19941520690918,\n",
              " 5.212177276611328,\n",
              " 5.466421127319336,\n",
              " 12.252511024475098,\n",
              " 6.706077575683594,\n",
              " 4.065589427947998,\n",
              " 4.653993606567383,\n",
              " 5.457876682281494,\n",
              " 3.086041212081909,\n",
              " 3.7151572704315186,\n",
              " 6.2192535400390625,\n",
              " 1.9719390869140625,\n",
              " 9.33389949798584,\n",
              " 5.115135192871094,\n",
              " 9.539637565612793,\n",
              " 10.086413383483887,\n",
              " 1.5194568634033203,\n",
              " 14.393999099731445,\n",
              " 10.234945297241211,\n",
              " 5.995419979095459,\n",
              " 6.917510986328125,\n",
              " 3.34240984916687,\n",
              " 5.238269329071045,\n",
              " 8.343894958496094,\n",
              " 1.8099308013916016,\n",
              " 3.9261083602905273,\n",
              " 10.05612850189209,\n",
              " 2.2527670860290527,\n",
              " 6.486404895782471,\n",
              " 2.9582700729370117,\n",
              " 5.185288906097412,\n",
              " 6.406308174133301,\n",
              " 5.7683281898498535,\n",
              " 3.0734403133392334,\n",
              " 5.218873977661133,\n",
              " 4.702879905700684,\n",
              " 4.394102096557617,\n",
              " 10.797359466552734,\n",
              " 11.863347053527832,\n",
              " 0.5926838517189026,\n",
              " 6.01500940322876,\n",
              " 7.169711589813232,\n",
              " 9.509917259216309,\n",
              " 4.242910385131836,\n",
              " 12.36166000366211,\n",
              " 4.465721130371094,\n",
              " 6.303833961486816,\n",
              " 9.476829528808594,\n",
              " 11.343690872192383,\n",
              " 4.872005939483643,\n",
              " 0.4718762934207916,\n",
              " 4.58466911315918,\n",
              " 6.183170795440674,\n",
              " 0.3252064883708954,\n",
              " 18.378873825073242,\n",
              " 3.85681414604187,\n",
              " 12.09811019897461,\n",
              " 5.41807746887207,\n",
              " 5.444289207458496,\n",
              " 7.333762168884277,\n",
              " 5.365969181060791,\n",
              " 4.289552688598633,\n",
              " 3.7955405712127686,\n",
              " 0.16386830806732178,\n",
              " 1.6380118131637573,\n",
              " 8.543694496154785,\n",
              " 3.87149715423584,\n",
              " 5.313150405883789,\n",
              " 3.0130393505096436,\n",
              " 5.059041500091553,\n",
              " 10.16032886505127,\n",
              " 4.538473129272461,\n",
              " 3.12898325920105,\n",
              " 6.4866766929626465,\n",
              " 7.0547776222229,\n",
              " 7.592052459716797,\n",
              " 2.7955949306488037,\n",
              " 2.9930012226104736,\n",
              " 5.3368659019470215,\n",
              " 3.7491979598999023,\n",
              " 1.979042649269104,\n",
              " 2.4564831256866455,\n",
              " 16.581077575683594,\n",
              " 2.8946518898010254,\n",
              " 5.055800437927246,\n",
              " 2.956840753555298,\n",
              " 5.001906394958496,\n",
              " 7.351319313049316,\n",
              " 10.23337459564209,\n",
              " 7.19080924987793,\n",
              " 8.807555198669434,\n",
              " 5.635375499725342,\n",
              " 5.831035137176514,\n",
              " 8.209003448486328,\n",
              " 4.341434001922607,\n",
              " 1.2298808097839355,\n",
              " 8.284222602844238,\n",
              " 0.00021871701756026596,\n",
              " 2.2024595737457275,\n",
              " 4.171783447265625,\n",
              " 2.242487668991089,\n",
              " 1.8193092346191406,\n",
              " 4.214334964752197,\n",
              " 5.408726215362549,\n",
              " 2.8777379989624023,\n",
              " 7.879416465759277,\n",
              " 7.777168273925781,\n",
              " 2.774547576904297,\n",
              " 5.511934280395508,\n",
              " 4.601036548614502,\n",
              " 0.4044710695743561,\n",
              " 3.6099119186401367,\n",
              " 8.20193099975586,\n",
              " 8.306145668029785,\n",
              " 12.38886833190918,\n",
              " 5.279142379760742,\n",
              " 3.5934813022613525,\n",
              " 9.845697402954102,\n",
              " 5.257843494415283,\n",
              " 5.763901710510254,\n",
              " 5.843574523925781,\n",
              " 6.142999172210693,\n",
              " 8.197342872619629,\n",
              " 5.969269275665283,\n",
              " 5.770363807678223,\n",
              " 4.87615966796875,\n",
              " 6.517145156860352,\n",
              " 3.4413695335388184,\n",
              " 4.776352882385254,\n",
              " 8.847034454345703,\n",
              " 5.787271499633789,\n",
              " 8.611276626586914,\n",
              " 9.243281364440918,\n",
              " 8.555243492126465,\n",
              " 5.64471960067749,\n",
              " 0.07014190405607224,\n",
              " 3.099217176437378,\n",
              " 4.511125564575195,\n",
              " 6.53361701965332,\n",
              " 4.786267280578613,\n",
              " 3.5167508125305176,\n",
              " 6.252538681030273,\n",
              " 5.912623405456543,\n",
              " 1.4382513761520386,\n",
              " 3.947028636932373,\n",
              " 6.0456013679504395,\n",
              " 5.08034610748291,\n",
              " 2.0313401222229004,\n",
              " 1.8091490268707275,\n",
              " 5.295377254486084,\n",
              " 7.838735580444336,\n",
              " 0.3516639471054077,\n",
              " 6.246742248535156,\n",
              " 5.826671600341797,\n",
              " 2.206538677215576,\n",
              " 3.518953800201416,\n",
              " 5.336509704589844,\n",
              " 5.911525726318359,\n",
              " 5.538269519805908,\n",
              " 5.867469310760498,\n",
              " 8.491436004638672,\n",
              " 9.050515174865723,\n",
              " 6.894087314605713,\n",
              " 10.503853797912598,\n",
              " 7.818398475646973,\n",
              " 7.90380859375,\n",
              " 3.230112314224243,\n",
              " 4.191224098205566,\n",
              " 7.343198299407959,\n",
              " 3.926792621612549,\n",
              " 7.948789119720459,\n",
              " 7.566359519958496,\n",
              " 5.737464427947998,\n",
              " 2.64139986038208,\n",
              " 7.5205512046813965,\n",
              " 6.426698684692383,\n",
              " 4.0063276290893555,\n",
              " 2.049989700317383,\n",
              " 7.38101863861084,\n",
              " 1.1733020544052124,\n",
              " 5.138535499572754,\n",
              " 3.029391050338745,\n",
              " 4.87376070022583,\n",
              " 4.55464506149292,\n",
              " 10.06173324584961,\n",
              " 6.886077880859375,\n",
              " 7.17410945892334,\n",
              " 4.102181911468506,\n",
              " 5.56067419052124,\n",
              " 5.943148612976074,\n",
              " 7.767138957977295,\n",
              " 9.24211597442627,\n",
              " 6.355541706085205,\n",
              " 2.0133652687072754,\n",
              " 5.234899520874023,\n",
              " 3.5037431716918945,\n",
              " 1.7810003757476807,\n",
              " 3.847059965133667,\n",
              " 2.796900749206543,\n",
              " 4.889506816864014,\n",
              " 4.758819103240967,\n",
              " 3.7138736248016357,\n",
              " 5.588699817657471,\n",
              " 9.682125091552734,\n",
              " 9.77613353729248,\n",
              " 8.597830772399902,\n",
              " 4.808538436889648,\n",
              " 3.525181293487549,\n",
              " 3.6274049282073975,\n",
              " 9.310877799987793,\n",
              " 8.938078880310059,\n",
              " 8.243486404418945,\n",
              " 2.310826301574707,\n",
              " 5.3948493003845215,\n",
              " 3.4669198989868164,\n",
              " 4.284926414489746,\n",
              " 8.208590507507324,\n",
              " 8.581363677978516,\n",
              " 7.808282852172852,\n",
              " 1.7882764339447021,\n",
              " 3.7876904010772705,\n",
              " 13.303934097290039,\n",
              " 5.429723262786865,\n",
              " 6.343317985534668,\n",
              " 13.082415580749512,\n",
              " 5.744436264038086,\n",
              " 3.8646442890167236,\n",
              " 3.894495725631714,\n",
              " 5.135791778564453,\n",
              " 9.516340255737305,\n",
              " 0.29852136969566345,\n",
              " 6.319019317626953,\n",
              " 5.311814308166504,\n",
              " 3.1637721061706543,\n",
              " 3.0231404304504395,\n",
              " 3.797034502029419,\n",
              " 8.04784107208252,\n",
              " 8.412434577941895,\n",
              " 0.42373529076576233,\n",
              " 7.503523826599121,\n",
              " 6.113607406616211,\n",
              " 1.440119743347168,\n",
              " 8.332588195800781,\n",
              " 7.656771183013916,\n",
              " 6.0493621826171875,\n",
              " 3.8633532524108887,\n",
              " 3.79784893989563,\n",
              " 11.063855171203613,\n",
              " 6.966440677642822,\n",
              " 3.6906955242156982,\n",
              " 6.058800220489502,\n",
              " 4.6096086502075195,\n",
              " 5.753396987915039,\n",
              " 0.6297328472137451,\n",
              " 0.0013835531426593661,\n",
              " 6.391210079193115,\n",
              " 15.885185241699219,\n",
              " 7.435353755950928,\n",
              " 3.1856236457824707,\n",
              " 2.479879856109619,\n",
              " 7.308261394500732,\n",
              " 4.920187950134277,\n",
              " 7.746853351593018,\n",
              " 1.26695716381073,\n",
              " 11.171951293945312,\n",
              " 3.475764274597168,\n",
              " 10.436206817626953,\n",
              " 8.761733055114746,\n",
              " 7.254984378814697,\n",
              " 3.685678720474243,\n",
              " 5.909708499908447,\n",
              " 0.658909022808075,\n",
              " 9.685036659240723,\n",
              " 0.820509672164917,\n",
              " 5.640264987945557,\n",
              " 13.174989700317383,\n",
              " 6.444137096405029,\n",
              " 9.589512825012207,\n",
              " 3.9493532180786133,\n",
              " 0.22791525721549988,\n",
              " 4.657925605773926,\n",
              " 7.091365337371826,\n",
              " 8.586688995361328,\n",
              " 11.736823081970215,\n",
              " 1.4465922117233276,\n",
              " 4.087557792663574,\n",
              " 6.76890230178833,\n",
              " 4.3847551345825195,\n",
              " 5.28729772567749,\n",
              " 6.946915149688721,\n",
              " 5.107124328613281,\n",
              " 3.6270947456359863,\n",
              " 6.201240062713623,\n",
              " 9.93529224395752,\n",
              " 4.460366725921631,\n",
              " 6.416500091552734,\n",
              " 4.01770544052124,\n",
              " 8.128716468811035,\n",
              " 1.8782457113265991,\n",
              " 5.279580116271973,\n",
              " 6.016691207885742,\n",
              " 6.274074554443359,\n",
              " 0.17147397994995117,\n",
              " 5.511912822723389,\n",
              " 6.571932315826416,\n",
              " 6.5116448402404785,\n",
              " 10.127824783325195,\n",
              " 8.010734558105469,\n",
              " 4.262451648712158,\n",
              " 1.189647912979126,\n",
              " 8.873729705810547,\n",
              " 2.1151440143585205,\n",
              " 10.561742782592773,\n",
              " 4.635789394378662,\n",
              " 5.742457389831543,\n",
              " 9.04294204711914,\n",
              " 4.526965141296387,\n",
              " 1.0315747261047363,\n",
              " 8.556751251220703,\n",
              " 4.061524868011475,\n",
              " 5.366591453552246,\n",
              " 5.108081817626953,\n",
              " 3.379755973815918,\n",
              " 1.4974912405014038,\n",
              " 4.687807083129883,\n",
              " 3.093755006790161,\n",
              " 2.9610393047332764,\n",
              " 4.809494495391846,\n",
              " 1.5085524320602417,\n",
              " 5.666281700134277,\n",
              " 11.016057014465332,\n",
              " 9.977781295776367,\n",
              " 7.043639659881592,\n",
              " 5.373755931854248,\n",
              " 1.411772608757019,\n",
              " 5.658066749572754,\n",
              " 9.679426193237305,\n",
              " 2.6519601345062256,\n",
              " 9.155475616455078,\n",
              " 7.218149662017822,\n",
              " 5.265575885772705,\n",
              " 11.891704559326172,\n",
              " 14.65470027923584,\n",
              " 6.627138137817383,\n",
              " 0.7807854413986206,\n",
              " 1.5131815671920776,\n",
              " 6.388283729553223,\n",
              " 4.4192328453063965,\n",
              " 9.415319442749023,\n",
              " 6.963683128356934,\n",
              " 1.406113862991333,\n",
              " 13.014314651489258,\n",
              " 6.308052062988281,\n",
              " 6.449732303619385,\n",
              " 12.757413864135742,\n",
              " 2.9273719787597656,\n",
              " 9.527804374694824,\n",
              " 2.0324759483337402,\n",
              " 4.8275556564331055,\n",
              " 5.066994667053223,\n",
              " 4.447919845581055,\n",
              " 6.631495952606201,\n",
              " 5.55120325088501,\n",
              " 8.078119277954102,\n",
              " 2.9063520431518555,\n",
              " 4.565713882446289,\n",
              " 10.866619110107422,\n",
              " 9.84593391418457,\n",
              " 7.7440185546875,\n",
              " 5.415408134460449,\n",
              " 5.133006572723389,\n",
              " 3.4230034351348877,\n",
              " 7.524741172790527,\n",
              " 5.418909072875977,\n",
              " 6.354885101318359,\n",
              " 4.990603923797607,\n",
              " 6.242613315582275,\n",
              " 14.556499481201172,\n",
              " 3.1430304050445557,\n",
              " 2.3480448722839355,\n",
              " 6.235404968261719,\n",
              " 4.26857328414917,\n",
              " 9.442070007324219,\n",
              " 4.909890174865723,\n",
              " 2.0156362056732178,\n",
              " 2.248762845993042,\n",
              " 4.649136543273926,\n",
              " 4.798343658447266,\n",
              " 5.882607936859131,\n",
              " 8.262413024902344,\n",
              " 7.376359939575195,\n",
              " 3.80942964553833,\n",
              " 2.6980998516082764,\n",
              " 0.44017648696899414,\n",
              " 11.761763572692871,\n",
              " 3.230440139770508,\n",
              " 6.508368968963623,\n",
              " 7.674038410186768,\n",
              " 4.043318748474121,\n",
              " 5.7379631996154785,\n",
              " 4.708330154418945,\n",
              " 7.996467590332031,\n",
              " 1.842368483543396,\n",
              " 4.524914741516113,\n",
              " 5.873254776000977,\n",
              " 5.313493728637695,\n",
              " 9.168160438537598,\n",
              " 11.403138160705566,\n",
              " 3.3598227500915527,\n",
              " 3.9347317218780518,\n",
              " 8.363566398620605,\n",
              " 3.8982675075531006,\n",
              " 8.436267852783203,\n",
              " 13.141243934631348,\n",
              " 10.802087783813477,\n",
              " 4.74680757522583,\n",
              " 6.294130802154541,\n",
              " 3.5951550006866455,\n",
              " 6.188207626342773,\n",
              " 1.8169496059417725,\n",
              " 3.9034852981567383,\n",
              " 5.510148048400879,\n",
              " 8.505091667175293,\n",
              " 7.1930999755859375,\n",
              " 6.150688648223877,\n",
              " 6.9217305183410645,\n",
              " 3.671964406967163,\n",
              " 4.004911422729492,\n",
              " 2.2646327018737793,\n",
              " 4.077027797698975,\n",
              " 6.130596160888672,\n",
              " 1.6980198621749878,\n",
              " 7.849879741668701,\n",
              " 7.301819801330566,\n",
              " 4.450442314147949,\n",
              " 10.943706512451172,\n",
              " 4.01707124710083,\n",
              " 1.5292696952819824,\n",
              " 6.504767417907715,\n",
              " 4.925691604614258,\n",
              " 7.417825698852539,\n",
              " 8.298260688781738,\n",
              " 8.979625701904297,\n",
              " 3.057417154312134,\n",
              " 0.8035396933555603,\n",
              " 2.1914126873016357,\n",
              " 3.4142541885375977,\n",
              " 5.041780471801758,\n",
              " 7.5014472007751465,\n",
              " 3.496941566467285,\n",
              " 6.781002998352051,\n",
              " 2.887573480606079,\n",
              " 4.062221527099609,\n",
              " 6.955216407775879,\n",
              " 6.920923233032227,\n",
              " 10.358687400817871,\n",
              " 5.574260234832764,\n",
              " 12.092263221740723,\n",
              " 2.463408946990967,\n",
              " 3.966111183166504,\n",
              " 8.326496124267578,\n",
              " 6.968217849731445,\n",
              " 5.980746269226074,\n",
              " 2.9814369678497314,\n",
              " 5.303589820861816,\n",
              " 4.0537824630737305,\n",
              " 5.3921661376953125,\n",
              " 3.5281403064727783,\n",
              " 4.2696661949157715,\n",
              " 2.029324531555176,\n",
              " 3.8742856979370117,\n",
              " 10.610527038574219,\n",
              " 12.536383628845215,\n",
              " 2.811537027359009,\n",
              " 7.637623310089111,\n",
              " 1.6767730712890625,\n",
              " 4.52787971496582,\n",
              " 7.647255897521973,\n",
              " 3.0793797969818115,\n",
              " 3.011970281600952,\n",
              " 8.500333786010742,\n",
              " 3.579237222671509,\n",
              " 5.972596645355225,\n",
              " 8.860884666442871,\n",
              " 3.7953763008117676,\n",
              " 9.403590202331543,\n",
              " 6.275900363922119,\n",
              " 2.1748783588409424,\n",
              " 5.622293472290039,\n",
              " 4.302709102630615,\n",
              " 9.081914901733398,\n",
              " 5.320755958557129,\n",
              " 2.0257980823516846,\n",
              " 6.254830837249756,\n",
              " 5.192052364349365,\n",
              " 10.933212280273438,\n",
              " 4.728914737701416,\n",
              " 9.595235824584961,\n",
              " 0.5796027183532715,\n",
              " 4.157682418823242,\n",
              " 4.016579627990723,\n",
              " 13.647113800048828,\n",
              " 5.9916534423828125,\n",
              " 11.849727630615234,\n",
              " 14.131911277770996,\n",
              " 6.5784010887146,\n",
              " 7.780639171600342,\n",
              " 2.506436347961426,\n",
              " 6.045741081237793,\n",
              " 5.9332451820373535,\n",
              " 16.535261154174805,\n",
              " 6.5789947509765625,\n",
              " 4.633293628692627,\n",
              " 2.419631004333496,\n",
              " 7.497753620147705,\n",
              " 9.948147773742676,\n",
              " 3.233557939529419,\n",
              " 8.84579849243164,\n",
              " 6.97996711730957,\n",
              " 10.91830825805664,\n",
              " 4.111715316772461,\n",
              " 8.715414047241211,\n",
              " 4.5062336921691895,\n",
              " 2.427407741546631,\n",
              " 7.951204299926758,\n",
              " 8.11351490020752,\n",
              " 5.023541450500488,\n",
              " 6.126648426055908,\n",
              " 4.553894996643066,\n",
              " 4.294673919677734,\n",
              " 0.6936216950416565,\n",
              " 2.789417266845703,\n",
              " 13.260258674621582,\n",
              " 7.460840225219727,\n",
              " 6.486573219299316,\n",
              " 3.6617016792297363,\n",
              " 6.945474624633789,\n",
              " 4.008434295654297,\n",
              " 2.891446828842163,\n",
              " 2.9840404987335205,\n",
              " 4.195545196533203,\n",
              " 7.022778511047363,\n",
              " 3.178630828857422,\n",
              " 8.8267240524292,\n",
              " 6.277919769287109,\n",
              " 10.116363525390625,\n",
              " 5.363240718841553,\n",
              " 5.922971725463867,\n",
              " 2.207144260406494,\n",
              " 5.503201007843018,\n",
              " 1.220487117767334,\n",
              " 4.260748863220215,\n",
              " 5.637580871582031,\n",
              " 17.360881805419922,\n",
              " 5.2448906898498535,\n",
              " 4.527775764465332,\n",
              " 9.719097137451172,\n",
              " 2.7161762714385986,\n",
              " 6.473772048950195,\n",
              " 3.608576774597168,\n",
              " 7.820332050323486,\n",
              " 8.197105407714844,\n",
              " 6.187785625457764,\n",
              " 9.70695972442627,\n",
              " 6.627692699432373,\n",
              " 5.45753812789917,\n",
              " 7.22779655456543,\n",
              " 7.419649600982666,\n",
              " 2.980830669403076,\n",
              " 9.40711784362793,\n",
              " 5.825328350067139,\n",
              " 5.772744178771973,\n",
              " 14.98463249206543,\n",
              " 2.8052568435668945,\n",
              " 1.903409719467163,\n",
              " 3.9546961784362793,\n",
              " 4.76403284072876,\n",
              " 5.30440616607666,\n",
              " 5.6715087890625,\n",
              " 5.596833229064941,\n",
              " 4.291349411010742,\n",
              " 7.027592182159424,\n",
              " 6.175100326538086,\n",
              " 3.130613327026367,\n",
              " 3.366147756576538,\n",
              " 6.421727657318115,\n",
              " 8.277274131774902,\n",
              " 5.7213311195373535,\n",
              " 8.410526275634766,\n",
              " 5.510217189788818,\n",
              " 5.936117172241211,\n",
              " 6.2665839195251465,\n",
              " 5.311181545257568,\n",
              " 7.973759174346924,\n",
              " 4.2273664474487305,\n",
              " 2.9146158695220947,\n",
              " 2.8562357425689697,\n",
              " 6.839139938354492,\n",
              " 6.165674209594727,\n",
              " 4.932563304901123,\n",
              " 6.038231372833252,\n",
              " 4.0418243408203125,\n",
              " 3.7599029541015625,\n",
              " 1.861736536026001,\n",
              " 7.591672420501709,\n",
              " 8.035048484802246,\n",
              " 8.173354148864746,\n",
              " 2.883333206176758,\n",
              " 2.1948463916778564,\n",
              " 2.8309977054595947,\n",
              " 3.20267653465271,\n",
              " 2.4397408962249756,\n",
              " 1.349751353263855,\n",
              " 6.804056644439697,\n",
              " 4.295994281768799,\n",
              " 3.965162515640259,\n",
              " 2.922138214111328,\n",
              " 6.342434883117676,\n",
              " 3.4515163898468018,\n",
              " 8.502765655517578,\n",
              " 5.880669116973877,\n",
              " 13.034778594970703,\n",
              " 3.669153928756714,\n",
              " 5.694609642028809,\n",
              " 11.784574508666992,\n",
              " 3.2030653953552246,\n",
              " 12.069496154785156,\n",
              " 6.919933795928955,\n",
              " 1.9554047584533691,\n",
              " 5.302900314331055,\n",
              " 7.883003234863281,\n",
              " 3.346337080001831,\n",
              " 13.209755897521973,\n",
              " 3.8756892681121826,\n",
              " 6.711246013641357,\n",
              " 10.519158363342285,\n",
              " 5.460628986358643,\n",
              " 2.6988840103149414,\n",
              " 6.791389465332031,\n",
              " 1.5989576578140259,\n",
              " 8.1192045211792,\n",
              " 2.0929181575775146,\n",
              " 3.8790833950042725,\n",
              " 2.723100185394287,\n",
              " 9.82665729522705,\n",
              " 5.10871696472168,\n",
              " 5.256776809692383,\n",
              " 2.9302244186401367,\n",
              " 9.263463020324707,\n",
              " 5.023797988891602,\n",
              " 6.513609886169434,\n",
              " 3.9045872688293457,\n",
              " 3.73948073387146,\n",
              " 11.802895545959473,\n",
              " 3.4923858642578125,\n",
              " 4.330825328826904,\n",
              " 0.7331990599632263,\n",
              " 7.102470397949219,\n",
              " 7.587382793426514,\n",
              " 8.702991485595703,\n",
              " 4.851277828216553,\n",
              " 3.8103795051574707,\n",
              " 2.4070987701416016,\n",
              " 5.829719543457031,\n",
              " 9.93591022491455,\n",
              " 7.343903541564941,\n",
              " 4.415614128112793,\n",
              " 2.531388521194458,\n",
              " 3.3796377182006836,\n",
              " 5.863293647766113,\n",
              " 11.741512298583984,\n",
              " 4.641272068023682,\n",
              " 2.164552927017212,\n",
              " 7.475106239318848,\n",
              " 12.414337158203125,\n",
              " 9.345294952392578,\n",
              " 6.861663818359375,\n",
              " 2.5049502849578857,\n",
              " 3.057978630065918,\n",
              " 2.9091763496398926,\n",
              " 8.396326065063477,\n",
              " 6.550691604614258,\n",
              " 8.14590072631836,\n",
              " 8.45132827758789,\n",
              " 8.674642562866211,\n",
              " 7.335646629333496,\n",
              " 3.9912397861480713,\n",
              " 0.9536605477333069,\n",
              " 7.758336544036865,\n",
              " 3.756234884262085,\n",
              " 3.10610032081604,\n",
              " 5.049346923828125,\n",
              " 6.090780258178711,\n",
              " 3.270839214324951,\n",
              " 6.1345367431640625,\n",
              " 2.1608662605285645,\n",
              " 7.784451961517334,\n",
              " 5.603485107421875,\n",
              " 7.175266742706299,\n",
              " 2.8507463932037354,\n",
              " 5.167179107666016,\n",
              " 6.336568832397461,\n",
              " 7.096854209899902,\n",
              " 4.889406204223633,\n",
              " 3.759510040283203,\n",
              " 5.4029541015625,\n",
              " 2.4124984741210938,\n",
              " 3.6187353134155273,\n",
              " 4.868170738220215,\n",
              " 6.0732574462890625,\n",
              " 3.419297218322754,\n",
              " 3.262594223022461,\n",
              " 3.2660109996795654,\n",
              " 6.428699493408203,\n",
              " 5.785438537597656,\n",
              " 5.045734405517578,\n",
              " 1.180722951889038,\n",
              " 1.0586024522781372,\n",
              " 1.2871906757354736,\n",
              " 1.6547236442565918,\n",
              " 4.319611072540283,\n",
              " 4.340904712677002,\n",
              " 14.30147647857666,\n",
              " 2.9883365631103516,\n",
              " 4.341948986053467,\n",
              " 9.334291458129883,\n",
              " 5.665292739868164,\n",
              " 7.26392936706543,\n",
              " 4.365898609161377,\n",
              " 4.83701753616333,\n",
              " 3.2776970863342285,\n",
              " 2.161404609680176,\n",
              " 4.957385540008545,\n",
              " 4.313692569732666,\n",
              " 8.923595428466797,\n",
              " 8.317009925842285,\n",
              " 2.454530954360962,\n",
              " 6.346419334411621,\n",
              " 7.346094608306885,\n",
              " 4.671327590942383,\n",
              " 5.580931663513184,\n",
              " 3.8123960494995117,\n",
              " 1.9273937940597534,\n",
              " 3.8263304233551025,\n",
              " 5.530033588409424,\n",
              " 5.369543552398682,\n",
              " 8.774797439575195,\n",
              " 1.0095831155776978,\n",
              " 5.678248405456543,\n",
              " 5.995609283447266,\n",
              " 8.028193473815918,\n",
              " 5.024048805236816,\n",
              " 1.3790192604064941,\n",
              " 5.419977188110352,\n",
              " 9.700263977050781,\n",
              " 6.892937660217285,\n",
              " 5.896727561950684,\n",
              " 1.7207063436508179,\n",
              " 14.644156455993652,\n",
              " 9.276480674743652,\n",
              " 11.394120216369629,\n",
              " 8.135120391845703,\n",
              " 6.249640464782715,\n",
              " 3.0160276889801025,\n",
              " 4.80823278427124,\n",
              " 6.904374122619629,\n",
              " 12.243645668029785,\n",
              " 4.916595458984375,\n",
              " 7.965065002441406,\n",
              " 3.463927745819092,\n",
              " 4.913549423217773,\n",
              " 8.512022018432617,\n",
              " 6.217001914978027,\n",
              " 4.428915023803711,\n",
              " 5.853476524353027,\n",
              " 0.0006064358749426901,\n",
              " 3.733778953552246,\n",
              " 1.9274400472640991,\n",
              " 4.124607563018799,\n",
              " 5.181486129760742,\n",
              " 4.758708953857422,\n",
              " 3.8757553100585938,\n",
              " 0.9927374720573425,\n",
              " 7.996576309204102,\n",
              " 4.415065288543701,\n",
              " 3.2491278648376465,\n",
              " 7.871076583862305,\n",
              " 10.306646347045898,\n",
              " 1.2830952405929565,\n",
              " 3.1305885314941406,\n",
              " 5.5517802238464355,\n",
              " 3.6567251682281494,\n",
              " 4.369134902954102,\n",
              " 5.206502437591553,\n",
              " 4.500938415527344,\n",
              " 3.5300018787384033,\n",
              " 7.467428207397461,\n",
              " 3.6162991523742676,\n",
              " 7.147615432739258,\n",
              " 1.2221271991729736,\n",
              " 2.6179277896881104,\n",
              " 6.844328880310059,\n",
              " 6.728200435638428,\n",
              " 5.21366024017334,\n",
              " 10.29270076751709,\n",
              " 7.33990478515625,\n",
              " 7.3178863525390625,\n",
              " 6.044707298278809,\n",
              " 5.550334930419922,\n",
              " 4.363170623779297,\n",
              " 2.787137269973755,\n",
              " 3.782773733139038,\n",
              " 6.664041996002197,\n",
              " 7.86622428894043,\n",
              " 4.875337600708008,\n",
              " 3.6790144443511963,\n",
              " 13.089375495910645,\n",
              " 0.9270748496055603,\n",
              " 8.865178108215332,\n",
              " 9.256707191467285,\n",
              " 3.812458038330078,\n",
              " 6.105600833892822,\n",
              " 3.9123964309692383,\n",
              " 10.025196075439453,\n",
              " 9.654003143310547,\n",
              " 7.1312079429626465,\n",
              " 6.055660724639893,\n",
              " 5.292233943939209,\n",
              " 1.4847235679626465,\n",
              " 1.522387146949768,\n",
              " 4.530259132385254,\n",
              " 5.82765531539917,\n",
              " 9.01311206817627,\n",
              " 8.307052612304688,\n",
              " 10.635381698608398,\n",
              " 9.082235336303711,\n",
              " 8.039507865905762,\n",
              " 7.028057098388672,\n",
              " 2.9422407150268555,\n",
              " 0.004722445271909237,\n",
              " 4.038548946380615,\n",
              " 6.429365158081055,\n",
              " 6.202670574188232,\n",
              " 0.46535512804985046,\n",
              " 5.29353141784668,\n",
              " 15.127914428710938,\n",
              " 10.269161224365234,\n",
              " 12.777652740478516,\n",
              " 6.414843559265137,\n",
              " 9.408455848693848,\n",
              " 2.5182456970214844,\n",
              " 11.949535369873047,\n",
              " 9.273468971252441,\n",
              " 7.920314311981201,\n",
              " 4.2568888664245605,\n",
              " 5.164667129516602,\n",
              " 4.599153518676758,\n",
              " 2.819376230239868,\n",
              " 6.266257286071777,\n",
              " 5.254677772521973,\n",
              " 4.490255355834961,\n",
              " 5.835187911987305,\n",
              " 4.46634578704834,\n",
              " 2.7819929122924805,\n",
              " 9.5911865234375,\n",
              " 6.907846450805664,\n",
              " 6.735779762268066,\n",
              " 1.4985463619232178,\n",
              " 4.709070205688477,\n",
              " 4.230742454528809,\n",
              " 8.917579650878906,\n",
              " 3.0860273838043213,\n",
              " 6.981604099273682,\n",
              " 3.8704211711883545,\n",
              " 6.948755741119385,\n",
              " 2.874572277069092,\n",
              " 3.487273931503296,\n",
              " 9.175549507141113,\n",
              " 7.941073894500732,\n",
              " 3.7513623237609863,\n",
              " 2.656083822250366,\n",
              " 5.503208160400391,\n",
              " 5.269097805023193,\n",
              " 9.7097749710083,\n",
              " 8.835604667663574,\n",
              " 3.460904359817505,\n",
              " 5.217325210571289,\n",
              " 5.51871919631958,\n",
              " 4.60899543762207,\n",
              " 10.913009643554688,\n",
              " 7.339128494262695,\n",
              " 4.972055435180664,\n",
              " 5.168416976928711,\n",
              " 4.708719188784016e-06,\n",
              " 0.23686444759368896,\n",
              " 2.901684522628784,\n",
              " 12.846004486083984,\n",
              " 6.573592662811279,\n",
              " 7.874410152435303,\n",
              " 4.288950443267822,\n",
              " 6.067219257354736,\n",
              " 4.9364471435546875,\n",
              " 1.7848609685897827,\n",
              " 0.014646247029304504,\n",
              " 10.507246971130371,\n",
              " 10.678043365478516,\n",
              " 5.724605083465576,\n",
              " 4.620944976806641,\n",
              " 8.041625022888184,\n",
              " 3.7708969116210938,\n",
              " 8.930984497070312,\n",
              " 7.956058979034424,\n",
              " 4.1892170906066895,\n",
              " 7.88470458984375,\n",
              " 9.004449844360352,\n",
              " 4.150972366333008,\n",
              " 6.748891830444336,\n",
              " 1.6253293752670288,\n",
              " 4.7715301513671875,\n",
              " 5.524335861206055,\n",
              " 7.464082717895508,\n",
              " 6.539798259735107,\n",
              " 10.097799301147461,\n",
              " 9.182781219482422,\n",
              " 2.3779962062835693,\n",
              " 6.015290260314941,\n",
              " 10.589999198913574,\n",
              " 4.9156293869018555,\n",
              " 7.6340765953063965,\n",
              " 5.98283052444458,\n",
              " 3.8738462924957275,\n",
              " 1.9866605997085571,\n",
              " 3.5919406414031982,\n",
              " 6.054117679595947,\n",
              " 5.419145584106445,\n",
              " 9.368915557861328,\n",
              " 8.896785736083984,\n",
              " 5.290619850158691,\n",
              " 8.913323402404785,\n",
              " 11.10485553741455,\n",
              " 4.2618513107299805,\n",
              " 11.656604766845703,\n",
              " 3.737638235092163,\n",
              " 2.4628090858459473,\n",
              " 13.053606986999512,\n",
              " 3.5982563495635986,\n",
              " 5.294641494750977,\n",
              " 6.614255905151367,\n",
              " 9.638794898986816,\n",
              " 5.972230434417725,\n",
              " 1.434937596321106,\n",
              " 4.704038619995117,\n",
              " 3.842242956161499,\n",
              " 2.9404420852661133,\n",
              " 6.127943515777588,\n",
              " 4.853212356567383,\n",
              " 7.726298809051514,\n",
              " 5.436012268066406,\n",
              " 8.812478065490723,\n",
              " 3.8516921997070312,\n",
              " 6.369900703430176,\n",
              " 3.33083176612854,\n",
              " 4.071264743804932,\n",
              " 4.277942180633545,\n",
              " 6.8375678062438965,\n",
              " 3.539964437484741,\n",
              " 6.354625225067139,\n",
              " 4.3487982749938965,\n",
              " 2.85276198387146,\n",
              " 6.518825054168701,\n",
              " 9.556160926818848,\n",
              " 2.4070844650268555,\n",
              " 5.120528697967529,\n",
              " 6.149238586425781,\n",
              " ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ov0eEwBNRLM",
        "colab_type": "code",
        "outputId": "eb0965b5-e4b3-4b87-996c-1fb80d04266c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "output.size()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 289
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13sZTA9ECu9w",
        "colab_type": "code",
        "outputId": "89db689f-7c2f-4d28-f6d8-4993feeefe06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(losses)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2339"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WH4v9Xgl0IZy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnFRlwSpO3CY",
        "colab_type": "code",
        "outputId": "80ad7066-2c2c-49ae-82ad-af07547307e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2028
        }
      },
      "source": [
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',    level=logging.INFO)\n",
        "\n",
        "def myhashfxn(obj):\n",
        "    return hash(obj) % (2 ** 32)\n",
        "\n",
        "# Set values for various parameters\n",
        "num_features = 400    # Word vector dimensionality                      \n",
        "min_word_count = 60   # Minimum word count                        \n",
        "num_workers = 4       # Number of threads to run in parallel\n",
        "context = 10          # Context window size                                                                                    \n",
        "downsampling = 1e-3   # Downsample setting for frequent words\n",
        "\n",
        "\n",
        "#**********************************************************\n",
        "\n",
        "\n",
        "\n",
        "# Initialize and train the model (this will take some time)\n",
        "from gensim.models import word2vec\n",
        "#model = word2vec.Word2Vec(hashfxn=myhashfxn)\n",
        "\n",
        "print (\"Training model...\")\n",
        "model = word2vec.Word2Vec(sentences, workers=num_workers,             size=num_features, min_count = min_word_count,             window = context, sample = downsampling,hashfxn=myhashfxn)\n",
        "\n",
        "# If you don't plan to train the model any further, calling \n",
        "# init_sims will make the model much more memory-efficient.\n",
        "model.init_sims(replace=True)\n",
        "\n",
        "# It can be helpful to create a meaningful model name and \n",
        "# save the model for later use. You can load it later using Word2Vec.load()\n",
        "model_name = \"300features_40minwords_10context\"\n",
        "model.save(model_name)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-06-06 02:15:44,719 : INFO : 'pattern' package not found; tag filters are not available for English\n",
            "2019-06-06 02:15:44,729 : INFO : collecting all words and their counts\n",
            "2019-06-06 02:15:44,730 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
            "2019-06-06 02:15:44,786 : INFO : PROGRESS: at sentence #10000, processed 225803 words, keeping 17776 word types\n",
            "2019-06-06 02:15:44,845 : INFO : PROGRESS: at sentence #20000, processed 451892 words, keeping 24948 word types\n",
            "2019-06-06 02:15:44,901 : INFO : PROGRESS: at sentence #30000, processed 671315 words, keeping 30034 word types\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training model...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2019-06-06 02:15:44,963 : INFO : PROGRESS: at sentence #40000, processed 897815 words, keeping 34348 word types\n",
            "2019-06-06 02:15:45,019 : INFO : PROGRESS: at sentence #50000, processed 1116963 words, keeping 37761 word types\n",
            "2019-06-06 02:15:45,073 : INFO : PROGRESS: at sentence #60000, processed 1338404 words, keeping 40723 word types\n",
            "2019-06-06 02:15:45,127 : INFO : PROGRESS: at sentence #70000, processed 1561580 words, keeping 43333 word types\n",
            "2019-06-06 02:15:45,181 : INFO : PROGRESS: at sentence #80000, processed 1780887 words, keeping 45714 word types\n",
            "2019-06-06 02:15:45,235 : INFO : PROGRESS: at sentence #90000, processed 2004996 words, keeping 48135 word types\n",
            "2019-06-06 02:15:45,289 : INFO : PROGRESS: at sentence #100000, processed 2226966 words, keeping 50207 word types\n",
            "2019-06-06 02:15:45,342 : INFO : PROGRESS: at sentence #110000, processed 2446580 words, keeping 52081 word types\n",
            "2019-06-06 02:15:45,397 : INFO : PROGRESS: at sentence #120000, processed 2668775 words, keeping 54119 word types\n",
            "2019-06-06 02:15:45,452 : INFO : PROGRESS: at sentence #130000, processed 2894303 words, keeping 55847 word types\n",
            "2019-06-06 02:15:45,504 : INFO : PROGRESS: at sentence #140000, processed 3107005 words, keeping 57346 word types\n",
            "2019-06-06 02:15:45,558 : INFO : PROGRESS: at sentence #150000, processed 3332627 words, keeping 59055 word types\n",
            "2019-06-06 02:15:45,613 : INFO : PROGRESS: at sentence #160000, processed 3555315 words, keeping 60617 word types\n",
            "2019-06-06 02:15:45,666 : INFO : PROGRESS: at sentence #170000, processed 3778655 words, keeping 62077 word types\n",
            "2019-06-06 02:15:45,718 : INFO : PROGRESS: at sentence #180000, processed 3999236 words, keeping 63496 word types\n",
            "2019-06-06 02:15:45,771 : INFO : PROGRESS: at sentence #190000, processed 4224449 words, keeping 64794 word types\n",
            "2019-06-06 02:15:45,828 : INFO : PROGRESS: at sentence #200000, processed 4448603 words, keeping 66087 word types\n",
            "2019-06-06 02:15:45,881 : INFO : PROGRESS: at sentence #210000, processed 4669967 words, keeping 67390 word types\n",
            "2019-06-06 02:15:45,934 : INFO : PROGRESS: at sentence #220000, processed 4894968 words, keeping 68697 word types\n",
            "2019-06-06 02:15:45,988 : INFO : PROGRESS: at sentence #230000, processed 5117545 words, keeping 69958 word types\n",
            "2019-06-06 02:15:46,042 : INFO : PROGRESS: at sentence #240000, processed 5345050 words, keeping 71167 word types\n",
            "2019-06-06 02:15:46,097 : INFO : PROGRESS: at sentence #250000, processed 5559165 words, keeping 72351 word types\n",
            "2019-06-06 02:15:46,151 : INFO : PROGRESS: at sentence #260000, processed 5779146 words, keeping 73478 word types\n",
            "2019-06-06 02:15:46,188 : INFO : collected 74218 word types from a corpus of 5920724 raw words and 266551 sentences\n",
            "2019-06-06 02:15:46,189 : INFO : Loading a fresh vocabulary\n",
            "2019-06-06 02:15:46,239 : INFO : effective_min_count=60 retains 6278 unique words (8% of original 74218, drops 67940)\n",
            "2019-06-06 02:15:46,240 : INFO : effective_min_count=60 leaves 5461149 word corpus (92% of original 5920724, drops 459575)\n",
            "2019-06-06 02:15:46,261 : INFO : deleting the raw counts dictionary of 74218 items\n",
            "2019-06-06 02:15:46,264 : INFO : sample=0.001 downsamples 50 most-common words\n",
            "2019-06-06 02:15:46,265 : INFO : downsampling leaves estimated 3933998 word corpus (72.0% of prior 5461149)\n",
            "2019-06-06 02:15:46,282 : INFO : estimated required memory for 6278 words and 400 dimensions: 23228600 bytes\n",
            "2019-06-06 02:15:46,282 : INFO : resetting layer weights\n",
            "2019-06-06 02:15:46,365 : INFO : training model with 4 workers on 6278 vocabulary and 400 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
            "2019-06-06 02:15:47,385 : INFO : EPOCH 1 - PROGRESS: at 10.38% examples, 406628 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:15:48,408 : INFO : EPOCH 1 - PROGRESS: at 21.29% examples, 413575 words/s, in_qsize 7, out_qsize 1\n",
            "2019-06-06 02:15:49,410 : INFO : EPOCH 1 - PROGRESS: at 32.44% examples, 421396 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:15:50,432 : INFO : EPOCH 1 - PROGRESS: at 43.62% examples, 423259 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:15:51,464 : INFO : EPOCH 1 - PROGRESS: at 54.97% examples, 424817 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:15:52,484 : INFO : EPOCH 1 - PROGRESS: at 66.10% examples, 425588 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:15:53,492 : INFO : EPOCH 1 - PROGRESS: at 77.17% examples, 426817 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:15:54,514 : INFO : EPOCH 1 - PROGRESS: at 87.80% examples, 425321 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:15:55,526 : INFO : EPOCH 1 - PROGRESS: at 99.28% examples, 426877 words/s, in_qsize 5, out_qsize 0\n",
            "2019-06-06 02:15:55,543 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2019-06-06 02:15:55,556 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2019-06-06 02:15:55,571 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2019-06-06 02:15:55,572 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-06-06 02:15:55,573 : INFO : EPOCH - 1 : training on 5920724 raw words (3934036 effective words) took 9.2s, 427695 effective words/s\n",
            "2019-06-06 02:15:56,590 : INFO : EPOCH 2 - PROGRESS: at 10.55% examples, 415013 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:15:57,597 : INFO : EPOCH 2 - PROGRESS: at 21.61% examples, 425098 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:15:58,603 : INFO : EPOCH 2 - PROGRESS: at 32.44% examples, 423699 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:15:59,616 : INFO : EPOCH 2 - PROGRESS: at 43.78% examples, 427531 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:16:00,655 : INFO : EPOCH 2 - PROGRESS: at 54.79% examples, 425298 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:16:01,651 : INFO : EPOCH 2 - PROGRESS: at 65.75% examples, 426312 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:16:02,657 : INFO : EPOCH 2 - PROGRESS: at 76.66% examples, 426679 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:16:03,666 : INFO : EPOCH 2 - PROGRESS: at 87.48% examples, 426696 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:16:04,667 : INFO : EPOCH 2 - PROGRESS: at 98.77% examples, 427844 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:16:04,722 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2019-06-06 02:16:04,730 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2019-06-06 02:16:04,743 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2019-06-06 02:16:04,751 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-06-06 02:16:04,752 : INFO : EPOCH - 2 : training on 5920724 raw words (3933412 effective words) took 9.2s, 429040 effective words/s\n",
            "2019-06-06 02:16:05,813 : INFO : EPOCH 3 - PROGRESS: at 10.71% examples, 403540 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:16:06,828 : INFO : EPOCH 3 - PROGRESS: at 21.95% examples, 419890 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:16:07,843 : INFO : EPOCH 3 - PROGRESS: at 32.76% examples, 419402 words/s, in_qsize 8, out_qsize 1\n",
            "2019-06-06 02:16:08,848 : INFO : EPOCH 3 - PROGRESS: at 43.95% examples, 423435 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:16:09,852 : INFO : EPOCH 3 - PROGRESS: at 54.97% examples, 424708 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:16:10,852 : INFO : EPOCH 3 - PROGRESS: at 65.75% examples, 424771 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:16:11,867 : INFO : EPOCH 3 - PROGRESS: at 77.00% examples, 426643 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:16:12,878 : INFO : EPOCH 3 - PROGRESS: at 87.47% examples, 424886 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:16:13,884 : INFO : EPOCH 3 - PROGRESS: at 98.77% examples, 426053 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:16:13,941 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2019-06-06 02:16:13,951 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2019-06-06 02:16:13,971 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2019-06-06 02:16:13,977 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-06-06 02:16:13,978 : INFO : EPOCH - 3 : training on 5920724 raw words (3933779 effective words) took 9.2s, 426823 effective words/s\n",
            "2019-06-06 02:16:14,994 : INFO : EPOCH 4 - PROGRESS: at 10.71% examples, 421947 words/s, in_qsize 8, out_qsize 1\n",
            "2019-06-06 02:16:16,005 : INFO : EPOCH 4 - PROGRESS: at 21.45% examples, 420339 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:16:17,043 : INFO : EPOCH 4 - PROGRESS: at 32.76% examples, 423055 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:16:18,064 : INFO : EPOCH 4 - PROGRESS: at 44.10% examples, 426267 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:16:19,067 : INFO : EPOCH 4 - PROGRESS: at 55.28% examples, 428252 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:16:20,097 : INFO : EPOCH 4 - PROGRESS: at 66.59% examples, 428933 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:16:21,104 : INFO : EPOCH 4 - PROGRESS: at 77.65% examples, 429764 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:16:22,126 : INFO : EPOCH 4 - PROGRESS: at 88.82% examples, 430430 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:16:23,091 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2019-06-06 02:16:23,096 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2019-06-06 02:16:23,098 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2019-06-06 02:16:23,099 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-06-06 02:16:23,103 : INFO : EPOCH - 4 : training on 5920724 raw words (3934080 effective words) took 9.1s, 431598 effective words/s\n",
            "2019-06-06 02:16:24,118 : INFO : EPOCH 5 - PROGRESS: at 10.71% examples, 423774 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:16:25,124 : INFO : EPOCH 5 - PROGRESS: at 21.64% examples, 425588 words/s, in_qsize 6, out_qsize 1\n",
            "2019-06-06 02:16:26,128 : INFO : EPOCH 5 - PROGRESS: at 32.60% examples, 427180 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:16:27,145 : INFO : EPOCH 5 - PROGRESS: at 43.78% examples, 428214 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:16:28,167 : INFO : EPOCH 5 - PROGRESS: at 55.12% examples, 429527 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:16:29,178 : INFO : EPOCH 5 - PROGRESS: at 66.26% examples, 430095 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:16:30,181 : INFO : EPOCH 5 - PROGRESS: at 77.17% examples, 430142 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:16:31,192 : INFO : EPOCH 5 - PROGRESS: at 88.31% examples, 431210 words/s, in_qsize 7, out_qsize 0\n",
            "2019-06-06 02:16:32,184 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
            "2019-06-06 02:16:32,198 : INFO : EPOCH 5 - PROGRESS: at 99.65% examples, 431646 words/s, in_qsize 2, out_qsize 1\n",
            "2019-06-06 02:16:32,199 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
            "2019-06-06 02:16:32,209 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
            "2019-06-06 02:16:32,214 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
            "2019-06-06 02:16:32,215 : INFO : EPOCH - 5 : training on 5920724 raw words (3933706 effective words) took 9.1s, 432312 effective words/s\n",
            "2019-06-06 02:16:32,216 : INFO : training on a 29603620 raw words (19669013 effective words) took 45.8s, 428987 effective words/s\n",
            "2019-06-06 02:16:32,217 : INFO : precomputing L2-norms of word weight vectors\n",
            "2019-06-06 02:16:32,267 : INFO : saving Word2Vec object under 300features_40minwords_10context, separately None\n",
            "2019-06-06 02:16:32,268 : INFO : not storing attribute vectors_norm\n",
            "2019-06-06 02:16:32,270 : INFO : not storing attribute cum_table\n",
            "2019-06-06 02:16:32,271 : WARNING : this function is deprecated, use smart_open.open instead\n",
            "2019-06-06 02:16:32,510 : INFO : saved 300features_40minwords_10context\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRHjnKP3O2-1",
        "colab_type": "code",
        "outputId": "94622392-c3b1-44af-9395-b5f3fd0d9149",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "source": [
        "model.doesnt_match(\"man woman child kitchen\".split())\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `doesnt_match` (Method will be removed in 4.0.0, use self.wv.doesnt_match() instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/models/keyedvectors.py:895: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
            "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'kitchen'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HrctvxdO26f",
        "colab_type": "code",
        "outputId": "d806b3e5-8cdc-4553-980b-bec741f50c0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        }
      },
      "source": [
        "\n",
        "model.most_similar(\"man\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('woman', 0.6654655933380127),\n",
              " ('doctor', 0.6339104175567627),\n",
              " ('boy', 0.6286064982414246),\n",
              " ('journalist', 0.594315767288208),\n",
              " ('businessman', 0.5905495285987854),\n",
              " ('lady', 0.5899538397789001),\n",
              " ('guy', 0.5748952627182007),\n",
              " ('soldier', 0.5712598562240601),\n",
              " ('scientist', 0.5709996223449707),\n",
              " ('priest', 0.5663358569145203)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jujvNDbPKve",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Vector Averaging\n",
        "import numpy as np # Make sure that numpy is imported\n",
        "def makeFeatureVec(words, model, num_features):\n",
        "# Function to average all of the word vectors in a given\n",
        "# paragraph\n",
        "#\n",
        "# Pre-initialize an empty numpy array (for speed)\n",
        "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
        "#\n",
        "    nwords = 0.\n",
        "#\n",
        "# Index2word is a list that contains the names of the words in\n",
        "# the model's vocabulary. Convert it to a set, for speed\n",
        "    index2word_set = set(model.wv.index2word)\n",
        "#\n",
        "# Loop over each word in the review and, if it is in the model's\n",
        "# vocaublary, add its feature vector to the total\n",
        "    for word in words:\n",
        "        if word in index2word_set:\n",
        "            nwords = nwords + 1.\n",
        "            featureVec = np.add(featureVec,model[word])\n",
        "#\n",
        "# Divide the result by the number of words to get the average\n",
        "    featureVec = np.divide(featureVec,nwords)\n",
        "    return featureVec\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c058MlCUPK3n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getAvgFeatureVecs(reviews, model, num_features):\n",
        "# Given a set of reviews (each one a list of words), calculate\n",
        "# the average feature vector for each one and return a 2D numpy array\n",
        "#\n",
        "# Initialize a counter\n",
        "    counter = 0\n",
        "#\n",
        "# Preallocate a 2D numpy array, for speed\n",
        "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
        "# Loop through the reviews\n",
        "    for review in reviews:\n",
        "        if counter%1000 == 0:\n",
        "            print (\"Review %d of %d\" % (counter, len(reviews)))\n",
        "        reviewFeatureVecs[counter] = makeFeatureVec(review, model,num_features)\n",
        "#\n",
        "# Increment the counter\n",
        "        counter = counter + 1\n",
        "    return reviewFeatureVecs\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjB4MMtqPLCN",
        "colab_type": "code",
        "outputId": "cfee0286-78b8-4ef6-9f2a-f8fe52a82d79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "clean_train_reviews = []\n",
        "for review in train1:\n",
        "    clean_train_reviews.append( review_to_wordlist( review,         remove_stopwords=True ))\n",
        "    \n",
        "trainDataVecs = getAvgFeatureVecs( clean_train_reviews, model, num_features )\n",
        "print (\"Creating average feature vecs for test reviews\")\n",
        "clean_test_reviews = []\n",
        "for review in test1:\n",
        "    clean_test_reviews.append( review_to_wordlist( review,         remove_stopwords=True ))\n",
        "\n",
        "\n",
        "# In[52]:"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-94-ad58fd9a1297>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mclean_train_reviews\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mreview\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mclean_train_reviews\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mreview_to_wordlist\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mreview\u001b[0m\u001b[0;34m,\u001b[0m         \u001b[0mremove_stopwords\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrainDataVecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetAvgFeatureVecs\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mclean_train_reviews\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_features\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train1' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajbzoLmrPK_I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def expand(lll):\n",
        "  s = []\n",
        "  for ll in lll:\n",
        "    for l in ll:\n",
        "      s.append(l)\n",
        "  return s   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DHiVhllPK0z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "l = [[1],[2,3],[4],[5],[5,5],[5],[5]]\n",
        "\n",
        "q = expand(l)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EiQPe8kYd8tc",
        "colab_type": "code",
        "outputId": "ff5754d4-8944-433b-ca6c-0a6f87c2e0e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(q)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 2, 3, 4, 5, 5, 5, 5, 5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDoM4k_bPKyo",
        "colab_type": "code",
        "outputId": "254610f6-0e82-4cec-e069-04c198483c36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "l = [[1],[2,3],[4],[5],[5,5],[5],[5]]\n",
        "q = expand(l)\n",
        "\n",
        "collections.Counter(q)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({1: 1, 2: 1, 3: 1, 4: 1, 5: 5})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3difQDQudksi",
        "colab_type": "code",
        "outputId": "b4c0bd3c-4050-45a5-ca73-d2ba41c1de12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "x0 = torch.randn(3, 4)\n",
        "x = nn.LogSoftmax(dim=1)(x0)\n",
        "x0, x"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[-1.5353,  0.3086, -2.6271,  1.6892],\n",
              "         [ 1.2561,  0.1278,  0.1149,  0.2243],\n",
              "         [ 0.1907, -0.0590,  0.6862, -0.7516]]),\n",
              " tensor([[-3.4904, -1.6464, -4.5822, -0.2659],\n",
              "         [-0.6928, -1.8211, -1.8340, -1.7246],\n",
              "         [-1.3376, -1.5873, -0.8422, -2.2800]]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 263
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "in88bM4EwK39",
        "colab_type": "code",
        "outputId": "2fc36fae-8595-4483-9769-17482c6ab86f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "x.size()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 4])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 265
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhBo3RYzfR5f",
        "colab_type": "code",
        "outputId": "0d5d52e7-d48d-4b45-dce1-366dad8d3c2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "y = torch.LongTensor(3).random_(4)\n",
        "y.size()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 267
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9iBvO8kKfTnn",
        "colab_type": "code",
        "outputId": "b1e3d0e3-19a9-4dbb-d95b-f004274d1fd4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "nn.NLLLoss()(x, y)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2.8944)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 268
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfd1YxBofVMz",
        "colab_type": "code",
        "outputId": "92a85bf5-1f3d-41b9-9749-45793abf6352",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "x.size()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPV5FKJxfYsy",
        "colab_type": "code",
        "outputId": "54b2b5c9-60ae-46b3-8991-7af8d61f83c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "y.size()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g12pBt-YfdAh",
        "colab_type": "code",
        "outputId": "8b0d43a6-b16a-4dfc-9637-366e87da1f4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "x.size()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 2])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 258
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEc8JhFosBoY",
        "colab_type": "code",
        "outputId": "58f54d6a-4926-486d-cb36-db8be03fb93d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "x[:,:,None].size()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 2, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 257
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4RxxrVZsEvj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}