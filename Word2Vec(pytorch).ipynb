{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled3.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7yYE7X7_mgS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras, os, pickle, re, sklearn, string, tensorflow\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from keras.utils import to_categorical\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "from keras.layers import Embedding\n",
        "from keras.optimizers import Adadelta\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "print('Keras version: \\t\\t%s' % keras.__version__)\n",
        "print('Scikit version: \\t%s' % sklearn.__version__)\n",
        "print('TensorFlow version: \\t%s' % tensorflow.__version__)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QK926j1NHzwm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download the punkt tokenizer for sentence splitting\n",
        "import nltk.data\n",
        "nltk.download('punkt')   \n",
        "\n",
        "# Load the punkt tokenizer\n",
        "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "\n",
        "# Define a function to split a review into parsed sentences\n",
        "def review_to_sentences( review, tokenizer, remove_stopwords=False ):\n",
        "    # Function to split a review into parsed sentences. Returns a \n",
        "    # list of sentences, where each sentence is a list of words\n",
        "    #\n",
        "    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
        "    raw_sentences = tokenizer.tokenize(review.strip())\n",
        "    #\n",
        "    # 2. Loop over each sentence\n",
        "    sentences = []\n",
        "    for raw_sentence in raw_sentences:\n",
        "        # If a sentence is empty, skip it\n",
        "        if len(raw_sentence) > 0:\n",
        "            # Otherwise, call review_to_wordlist to get a list of words\n",
        "            sentences.append( review_to_wordlist( raw_sentence,remove_stopwords ))\n",
        "    #\n",
        "    # Return the list of sentences (each sentence is a list of words,\n",
        "    # so this returns a list of lists\n",
        "    return sentences\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6i2vWZB-JmoV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')# Download text data sets, including stop words\n",
        "from nltk.corpus import stopwords # Import the stop word list\n",
        "import re\n",
        "from bs4 import BeautifulSoup"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhSC5kt8MJ8z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import various modules for string cleaning\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "def review_to_wordlist( review, remove_stopwords=False ):\n",
        "# Function to convert a document to a sequence of words,\n",
        "# optionally removing stop words. Returns a list of words.\n",
        "#\n",
        "# 1. Remove HTML\n",
        "    review_text = BeautifulSoup(review).get_text()\n",
        "#\n",
        "# 2. Remove non-letters\n",
        "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
        "#\n",
        "# 3. Convert words to lower case and split them\n",
        "    words = review_text.lower().split()\n",
        "#\n",
        "# 4. Optionally remove stop words (false by default)\n",
        "    if remove_stopwords:\n",
        "        stops = set(stopwords.words(\"english\"))\n",
        "        words = [w for w in words if not w in stops]\n",
        "#\n",
        "# 5. Return a list of words\n",
        "    return(words)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4Bi8oMQNTg0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download the punkt tokenizer for sentence splitting\n",
        "import nltk.data\n",
        "nltk.download('punkt')\n",
        "# Load the punkt tokenizer\n",
        "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "# Define a function to split a review into parsed sentences\n",
        "def review_to_sentences( review, tokenizer, remove_stopwords=False ):\n",
        "# Function to split a review into parsed sentences. Returns a\n",
        "# list of sentences, where each sentence is a list of words\n",
        "#\n",
        "# 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
        "    raw_sentences = tokenizer.tokenize(review.strip())\n",
        "#\n",
        "# 2. Loop over each sentence\n",
        "    sentences = []\n",
        "    for raw_sentence in raw_sentences:\n",
        "# If a sentence is empty, skip it\n",
        "        if len(raw_sentence) > 0:\n",
        "# Otherwise, call review_to_wordlist to get a list of words\n",
        "            sentences.append( review_to_wordlist( raw_sentence, remove_stopwords ))\n",
        "#\n",
        "# Return the list of sentences (each sentence is a list of words,\n",
        "# so this returns a list of lists\n",
        "    return sentences\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6DIOdTyO3IW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_files(path):\n",
        "    documents = list()\n",
        "# Read in all files in directory\n",
        "    if os.path.isdir(path):\n",
        "        for filename in os.listdir(path):\n",
        "            with open('%s/%s' % (path, filename),encoding='utf-8') as f:\n",
        "                doc = f.read()\n",
        "                # doc = clean_text(doc)\n",
        "                documents.append(doc)\n",
        "# Read in all lines in a txt file\n",
        "    if os.path.isfile(path):\n",
        "        with open(path, encoding='iso-8859-1') as f:\n",
        "            doc = f.readlines()\n",
        "            for line in doc:\n",
        "                documents.append(line)\n",
        "    return documents\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43DtcD11O3Gi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pos_train = read_files('/home/workspace/aclImdb/train/pos')\n",
        "neg_train = read_files('/home/workspace/aclImdb/train/neg')\n",
        "pos_test = read_files('/home/workspace/aclImdb/test/pos')\n",
        "neg_test = read_files('/home/workspace/aclImdb/test/neg')\n",
        "train1 = pos_train + neg_train\n",
        "test1 = pos_test + neg_test\n",
        "\n",
        "#docs = negative_docs + positive_docs\n",
        "l_train = [1 for _ in range(len(pos_train))] + [0 for _ in range(len(neg_train))]\n",
        "l_test = [1 for _ in range(len(pos_train))] + [0 for _ in range(len(neg_test))]\n",
        "train = np.column_stack((train1,l_train))\n",
        "test = np.column_stack((test1,l_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49_bGNsNO3Ek",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences = [] # Initialize an empty list of sentences\n",
        "num_reviews = 25000\n",
        "print (\"Parsing sentences from training set\")\n",
        "for review in range( 0, num_reviews ):\n",
        "    sentences += review_to_sentences(train[review][0], tokenizer)\n",
        "'''\n",
        "print (\"Parsing sentences from unlabeled set\")\n",
        "for review in unlabeled_train[\"review\"]:\n",
        "    sentences += review_to_sentences(review, tokenizer)\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnFRlwSpO3CY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',    level=logging.INFO)\n",
        "\n",
        "def myhashfxn(obj):\n",
        "    return hash(obj) % (2 ** 32)\n",
        "\n",
        "# Set values for various parameters\n",
        "num_features = 400    # Word vector dimensionality                      \n",
        "min_word_count = 60   # Minimum word count                        \n",
        "num_workers = 4       # Number of threads to run in parallel\n",
        "context = 10          # Context window size                                                                                    \n",
        "downsampling = 1e-3   # Downsample setting for frequent words\n",
        "\n",
        "\n",
        "#**********************************************************\n",
        "\n",
        "\n",
        "\n",
        "# Initialize and train the model (this will take some time)\n",
        "from gensim.models import word2vec\n",
        "#model = word2vec.Word2Vec(hashfxn=myhashfxn)\n",
        "\n",
        "print (\"Training model...\")\n",
        "model = word2vec.Word2Vec(sentences, workers=num_workers,             size=num_features, min_count = min_word_count,             window = context, sample = downsampling,hashfxn=myhashfxn)\n",
        "\n",
        "# If you don't plan to train the model any further, calling \n",
        "# init_sims will make the model much more memory-efficient.\n",
        "model.init_sims(replace=True)\n",
        "\n",
        "# It can be helpful to create a meaningful model name and \n",
        "# save the model for later use. You can load it later using Word2Vec.load()\n",
        "model_name = \"300features_40minwords_10context\"\n",
        "model.save(model_name)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRHjnKP3O2-1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.doesnt_match(\"man woman child kitchen\".split())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HrctvxdO26f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "model.most_similar(\"man\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jujvNDbPKve",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Vector Averaging\n",
        "import numpy as np # Make sure that numpy is imported\n",
        "def makeFeatureVec(words, model, num_features):\n",
        "# Function to average all of the word vectors in a given\n",
        "# paragraph\n",
        "#\n",
        "# Pre-initialize an empty numpy array (for speed)\n",
        "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
        "#\n",
        "    nwords = 0.\n",
        "#\n",
        "# Index2word is a list that contains the names of the words in\n",
        "# the model's vocabulary. Convert it to a set, for speed\n",
        "    index2word_set = set(model.wv.index2word)\n",
        "#\n",
        "# Loop over each word in the review and, if it is in the model's\n",
        "# vocaublary, add its feature vector to the total\n",
        "    for word in words:\n",
        "        if word in index2word_set:\n",
        "            nwords = nwords + 1.\n",
        "            featureVec = np.add(featureVec,model[word])\n",
        "#\n",
        "# Divide the result by the number of words to get the average\n",
        "    featureVec = np.divide(featureVec,nwords)\n",
        "    return featureVec\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c058MlCUPK3n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getAvgFeatureVecs(reviews, model, num_features):\n",
        "# Given a set of reviews (each one a list of words), calculate\n",
        "# the average feature vector for each one and return a 2D numpy array\n",
        "#\n",
        "# Initialize a counter\n",
        "    counter = 0\n",
        "#\n",
        "# Preallocate a 2D numpy array, for speed\n",
        "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
        "# Loop through the reviews\n",
        "    for review in reviews:\n",
        "        if counter%1000 == 0:\n",
        "            print (\"Review %d of %d\" % (counter, len(reviews)))\n",
        "        reviewFeatureVecs[counter] = makeFeatureVec(review, model,num_features)\n",
        "#\n",
        "# Increment the counter\n",
        "        counter = counter + 1\n",
        "    return reviewFeatureVecs\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjB4MMtqPLCN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clean_train_reviews = []\n",
        "for review in train1:\n",
        "    clean_train_reviews.append( review_to_wordlist( review,         remove_stopwords=True ))\n",
        "    \n",
        "trainDataVecs = getAvgFeatureVecs( clean_train_reviews, model, num_features )\n",
        "print (\"Creating average feature vecs for test reviews\")\n",
        "clean_test_reviews = []\n",
        "for review in test1:\n",
        "    clean_test_reviews.append( review_to_wordlist( review,         remove_stopwords=True ))\n",
        "\n",
        "\n",
        "# In[52]:"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajbzoLmrPK_I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DHiVhllPK0z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDoM4k_bPKyo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}